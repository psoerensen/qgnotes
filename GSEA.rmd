---
title: "Gene Set Enrichment Analysis"
author: "Palle Duun Rohde, Stefan McKinnon Høj-Edwards, Izel Fourie Sørensen & Peter Sørensen"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    citation_package: natbib
    number_sections: yes
    includes:
      in_header: preamble.tex
  html_document:
    number_sections: yes
    includes:
      in_header: mathjax_header.html
  word_document: default
bibliography: [qg2021.bib, book.bib, packages.bib]
link-citations: yes
---
# Introduction
From association studies it has been shown that the markers associated with trait variation are not uniformly distributed throughout the genome, but are enriched in genes that are connected in biological pathways \citep{Allen2010,Lage2012,Maurano2012,ORoak2012}. This knowledge could be used to construct a statistical modelling framework which quantifies the joint effect of a set of markers located within \textit{e.g.} genes, sequence ontology, biological pathways, protein interactions, or any other type of externally, prior biological knowledge. Such SNP sets can be termed \textbf{genomic features}.

The methodology that collectively tests a set of genome-wide genetic markers for association with phenotypic variation is known as \textbf{gene set enrichment analyses} (GSEA) \citep{Wang2007,Listgarten2013}. [SEE THE SUGGESTION THAT FOLLOWS The idea of aggregating smaller units into larger sets was originally inspired by gene expression microarray analyses, where individually differential expressed genes are of minor interest, instead the focus is on identifying patterns of differentially expression by aggregating genes exhibiting similarity in their functional annotation \citep{Goeman2004,Subramanian2005, Goeman2007}.] [SUGGESTION: The idea of aggregating smaller units into larger sets was originally inspired by gene expression microarray analyses, where the focus is on identifying patterns of differential expression by aggregating genes that exhibit similarity in their functional annotation \citep{Goeman2004,Subramanian2005, Goeman2007}.]  Various GSEA approaches have been developed through the years and have been reviewed extensively, see \textit{e.g.} \citep{Wang2010,Fridley2011,Mooney2014,DeLeeuw2016}. Common for all these approaches are the test for association between trait variation and the joint contribution of multiple genetic variants aggregated within predefined sets, \textit{i.e.}, genomic features.

Genomic features are collections of genetic variants grouped together based on common biological- or molecular functions, or other characteristics. The aggregation of genetic variants rely on prior biological knowledge from external sources such as protein-protein interactions (\textit{e.g.}, STRING \citep{VonMering2005}), biological pathways (\textit{e.g.}, KEGG \citep{Kanehisa2000}), gene functions (\textit{e.g.}, gene ontology (GO) terms \citep{Ashburner2000}), sequence ontologies (\textit{e.g.}, introns, exon and binding sites \citep{Eilbeck2005}), drug targets (\textit{e.g.}, drug bank \citep{Wishart2006}), genome-wide expression patterns (\textit{e.g.}, GTEx \citep{GTEx2015}), or prior trait associations (\textit{e.g.}, human GWAS catalog \citep{Buniello2019}).  In addition, feature sets can be created from other types of omics data such as metabolomic, proteomic or epigenetic variation.

Naturally, only genetic markers located within the genomic features [OR FEATURE? NOT SURE WHETHER IT SHOULD BE PLURAL] can be considered in the analysis, thus, an important step is the mapping of variants to the genomic features [SEE COMMENT ABOVE] (Figure \ref{fig:GF}). This is typically done by grouping all markers within known gene regions. To capture regulatory regions for each gene, upstream and downstream regions are often included, and potentially also any regions in linkage disequilibrium (LD) with the gene. Therefore, some markers may be linked to multiple feature sets.

\begin{figure}[h]
\centering
\includegraphics[width=.95\textwidth]{genomic_features_updated_2}
\caption{Graphical representation of genomic feature classes. First, all SNPs located within the same gene region (\textit{e.g.}~the transcribed region, dark-blue SNPs) are aggregated. Gene regions can be extended such that SNPs within regulatory regions are included (light-blue SNPs). [Second, genes can then be grouped based on prior biological information, [CONSIDER TO USE \textit{i.e.} genomic features, INSTEAD OF: \textit{genomic features}, such as genes connected in pathways, or based on other similarities, such as protein networks or common metabolite signatures.] [OR: Second, genes can then be grouped into genomic features based on prior biological information, such as genes connected in pathways, or based on other similarities, such as protein networks or common metabolite signatures.] The genomic feature classes are thus collections of SNPs that have been aggregated based on shared biological or molecular characteristics.}
\label{fig:GF}
\end{figure}

The degree of new knowledge obtained from the genomic feature analysis strongly depends on the quality and complexity of the genomic feature class. The more reliable the resource is, the more accurate the [following] results will be. However, if the degree of feature complexity is low, and the quality is high, the outcome might be of minor interest, \textit{e.g.} chromosomal regions are a well-defined feature, but enrichment of certain chromosomes or chromosomal regions might be less interesting. It is therefore important, prior to the feature analysis, to clearly formulate the scope of the analysis. 

Genomic feature models have the prospect to contribute with novel knowledge, but they highly depend upon the availability and specificity of the prior biological information. Unfortunately, such information is not readily available across the tree of life. For model organisms and humans, much information is available, but even for well-studies organisms, such as livestock species, the amount and level of detail is limited. However, the definitions of genomic features are constantly evolving, and new knowledge will continuously be added, also for those organisms which currently are lacking good feature information.


## Different GSEA modelling approaches
[The] GSEA test[s] can be categorized as belonging to either a \emph{Single-step} or a \emph{Two-step} approach. In the single-step approaches, a genomic feature is modeled by a single model. The estimated effects are then evaluated, either by the properties of the model (\textit{e.g.}, score based statistics) or by comparing the model to a null hypothesis. The set of markers are modeled as a \emph{joint} contribution to a phenotypic trait, by including them as an extra random effect. In the two-step approaches, a single model is used to calculate test statistics on all the markers' effects by linear regression or linear mixed models.[(\textit{i.e.}, from linear regression or linear mixed models)]. The test statistics [does this refer to the two-step approach? The two-step approach sounds shorter than the single step approach?] described below all attempt to determine whether a given set of genetic variants contributes to the observed phenotypic trait. [I can not see test statistics decsribed below - perhaps write: the test statistics in section/chapter 2 all attempt...]

### Null hypotheses
We distinguish between two types of null hypotheses, the \emph{competitive} and the \emph{self-contained} \citep{Goeman2007,Maciejewski2013}. The self-contained is the easiest and corresponds to determining whether a genomic feature, [by it self], does not display  any association to the phenotypic trait. This is usually done by defining that the variance component or predicted effect equals zero.

The competitive corresponds to determining whether the degree of association within a genomic feature is the same as outside the genomic feature.

Naturally, the choice of null hypothesis affects the choice of test statistic, but also the biological interpretation of the significance of a finding. The self-contained may be preferable over the competitive null hypothesis, as it has more power \citep{Goeman2007}, and the biological interpretation is simpler as it determines whether there is [association or not] or there is no association. [The self-contained may be preferable over the competitive null hypothesis, as it has more power \citep{Goeman2007}, and the biological interpretation is simpler as it determines whether the genomic feature is associated with the trait or not.]

### Evaluating the test statistics
Once a test statistic has been calculated, it needs to be evaluated to determine whether the genomic feature of interest is significant. This is done by finding the test statistic's position within a distribution, allowing us to evaluate the probability of finding a test statistic of the given magnitude by chance.

We distinguish between three types of distributions;
the exact,
the approximate,
and the empirical distribution.

The \indx{exact distributions} (e.g.\ hypergeometric test) are derived from the test statistic itself. [They might seem to be the preferred, but only if the test statistic actually does describe the desired property being tested.]

The \indx{approximate distributions} (e.g.\ $\chi^2$) relies on that some distributions approximate each other under certain conditions. [We can then replace an intangible expression with a simpler, but when being applied to actual data, the conditions are [`bent' - BEND] into place.] THE INTANGIBLE EXPRESSION CAN THEN BE REPLACED WITH A SIMPLER EXPRESSION

The \indx{empirical distributions} are the brute-force `when-all-else-fails' solutions we attend [turn] to, when the other distributions are too computationally demanding, or the conditions for approximating seem to [too] strongly bent. Usual methods for obtaining these are bootstrapping or permutation routines, but caution should be taken under which conditions the routines are performed.

# Single-step approaches
In the single-step approaches, the data is fitted to the model [which model?] with [AI?]-Restricted Maximum Likelihood (AI-REML) [should this abbreviation be written out first?] to obtain estimates of the variance components ($\hat{\theta}$) and the likelihood, which is used to compare nested models with a likelihood ratio test (LRT). [are the variance compoments and likelihood used __to compare__ nested models with a likelihood ratio test? - which are the nested models?]

The LRT, Wald's test, and Rao's Score test, can be referred to as `The Holy Trinity' \citep{Rao2009}, and are all related to the likelihood and the first and second derivation (Figure \ref{fig:gfm:trinity}). The first derivative gives the slope of the function\footnote{The first derivative of the likelihood is also referred to as the 'score', which is the basis of the Score based statistic.}, and the second derivative is related to the uncertainty of the estimated variance component.
[in the figure "Likelihood Ratio Test" - is missing the "t"]

\begin{figure}
  \centering
	\includegraphics[width=.5\textwidth]{trinity}
  \caption[The Holy Trinity]{
    \figcap{
    The Holy Trinity of Likelihood Ratio, Wald's and Rao's Score test.}
    The graph displays likelihood as a function of the variance components, maximised at the true value, $\thetahat$.
  }
  \label{fig:gfm:trinity}
\end{figure}  

The LRT compares the model fit between the full model and the reduced model. In Wald's test the model parameters are fitted using the full model, and [it?] test[s] whether the estimated variance component is significantly different from a particular value (usually zero). Rao's Score test uses the reduced model (\textit{i.e.}, null model), and estimates the size of improvement in model fit if an additional variance component was added to the model. Both the Wald and the Rao's score tests are asymptotically equivalent to the LRT, that is, as the sample size becomes infinitely large, the values of the Wald and Rao's score test statistics will become increasingly close to the test statistic from the LRT [OR: "increasingly close to the LRT (test?) statistic"]. A few additional details on the different approaches are given below.

## Likelihood Ratio Test
The LRT is used to assess whether a reduced model fits the data better than the full model
by comparing the likelihoods of the two models. A high LR indicates that the full model is better at explaining the observed genomic variance than the reduced model with one less variance component. The reduced model has to be nested within the full model, and when REML is used, the two models being compared has to have the same fixed effects, otherwise the two likelihoods are not comparable. The LRT statistic can be derived as:

\begin{equation}
T_\textrm{LRT} = 2 \ln\left[\frac{L(\hat{\theta}|\vect{y})}{L(\hat{\theta}_r|\vect{y})}\right] 
		= -2\left[l(\hat{\theta}_r|\vect{y})-l(\hat{\theta}|\vect{y})\right],
\end{equation}
where $l(\hat{\theta}|{y})$ is the log-likelihood for the full model, and $l(\hat{\theta}_r|{y})$ is the log-likelihood for the reduced model. When the sample size is sufficiently large, the LRT statistic is $\chi^2$ distributed with $\kappa$ degrees of freedom, where $\kappa$ is the difference in parameters between the two comparable models.

## Wald's test
The Wald's test is a parametric test that compares an estimated variance component to some particular value, $\theta_0$, based on a null hypothesis.

\begin{equation}
  \frac{(\thetahat - \theta_0)^2}{\Var(\thetahat)}.
  \label{eq:walds.test.basic}
\end{equation}

The test statistic is assumed $\chi^2$-distributed with one degree of freedom. If the null hypothesis was that the {i\th} variance component was equal to zero, the above can be expressed as a quadratic form by

\begin{equation}
  T_\textrm{Wald} = (\thetahat_i - 0)' \left[ \matr{I}_E (\thetahat) ^{-1} \right]^{ii} (\thetahat_i - 0),
  \label{eq:walds.test}
\end{equation}

where $\left[ \matr{I}_E (\thetahat) ^{-1} \right]^{ii}$ is the {i\th} diagonal element of the inverse expected information matrix. Wald's test has the advantage that it only requires fitting and estimating the parameters under the full model. If the test fails to reject the null hypothesis, this suggests that removing the corresponding variance component from the model will not substantially harm the fit of that model. 

Wald's test is computed as the parameter estimate divided by its asymptotic standard error. The asymptotic standard errors are computed from the inverse of the second derivative matrix of the likelihood with respect to each of the covariance parameters. The Wald's test is valid for large samples, but it can be unreliable for small data sets. When used on correlated variance components, $I_E$ might not be full rank and therefore not invertible. 

## Rao's Score test
Rao's score test requires estimating only a single model that does not include the parameter(s) of interest. Thus, one can test if adding the variance component to the model will result in a significant improvement in model fit, without fitting additional models.  The test statistic is based on the slope (or score) of the likelihood function, using model parameters estimated under the null model. If the null model is true, then the slope of the likelihood function is close to zero. If the null model is not true, fixing a variance component to a value will penalise the likelihood.

Instead of calculating likelihoods for both the null and the full model,  the first and second derivatives are used to get an indication of the produced change. The Rao's Score test statistic can be formulated as:

\begin{equation}
  T_\textrm{Rao} = \left(l'(\theta_i = 0, \thetahat_{-1}) \right)' \left[ \matr{I}_E (\theta_i = 0, \thetahat_{-1}) ^{-1} \right]^{ii} \left(l'(\theta_i = 0, \thetahat_{-1}) \right),
\end{equation}

where $\left(l'(\theta_i = 0, \thetahat_{-i}) \right)$ is the first derivative of the \emph{full model's} likelihood function, calculated using the parameters estimated with the \emph{null model} and the parameter of [interest ($\theta_i$) fixed cf.\ null model ???]. $\left[ \matr{I}_E (\theta_i = 0, \thetahat_{-1}) ^{-1} \right]^{ii}$ is the {i\th} diagonal element of the inverse expected information matrix, under the same conditions as the first derivative. It is possible to use the average between the expected and observed information matrix, i.e.\ the average information matrix, as it may be easier to compute \citep{Freedman2007,Johnson1995,Madsen1994,Jensen1997}.

The Rao's Score test has an asymptotic distribution of $\chi^2$ with number of parameters in $\thetahat_i$ as degrees of freedom when the null hypothesis is true. Some issues related to the test statistic may occur if the information matrix is not positive definite which can happen if the null hypothesis is true \citep{Freedman2007}.

## Score based statistics
There are several different score-based statistics that also are derived from the first derivative of the likelihood. The score statistic can be written as

\begin{equation}
  T_\textrm{Score} = \half (\vect{y} - \matr{X}\bfbeta)' \matr{V}^{-1}\matr{Z}\vect{g}_i\matr{Z'V}^{-1}(\vect{y}-\matr{X}\bfbeta),
\end{equation}  

which under the null hypothesis $H_0: \sigma^2_i = 0$ should be close to zero. If the parameters are estimated under the null model, the score statistic for a group of markers $i$ is:

\begin{equation}
  T_\textrm{Score} = \half (\vect{y} - \matr{X}\betahat)' \Vhat^{-1}_0\matr{Z}\vect{g}_i\matr{Z}'\Vhat^{-1}_0(\vect{y}-\matr{X}\betahat).
\end{equation}  

Utilizing that $\Phat\vect{y} = \Vhat^{-1}_0 (\vect{y}-\matr{X}\betahat) = \ehat$, $T_i$ can be computed as:

\begin{equation}
  T_{Score} = \half \ehat' \matr{Z G}_i \matr{Z}' \ehat = \half \ehat' \matr{Z} \frac{\matr{W}_i \matr{W}_i'}{m_i} \matr{Z}' \ehat,
  \label{eq:Ti}
\end{equation}

where the latter expansion is done for the subset of markers. This is computationally simple, and also easy to derive an empirical distribution of the score statistic under both the competitive and self-contained null hypothesis. 


# Two-step approaches
In the first step, a test statistic for the association (\textit{e.g.}, t-statistic[s]) of individual markers with the trait phenotype is obtained from traditional single-marker regression (it can also be obtained from a  mixed model or a Bayesian linear regression). In the second step, for each set of markers being tested, a summary statistic is obtained. [In the second step, a summary statistic is obtained for each of the markers being tested.] For each set an appropriate summary statistic measuring the degree of association between the set of markers and the phenotypes is computed. [does this mean the same as the first description of a two-step approach? - see section 1.1] 

## Gene set statistics
Determination of the association of individual markers with the phenotype is based on a single marker test statistic such as the t-statistic and a threshold for this statistic.

Let $m$ denote the total number of markers tested, $m_F$ is the total number of markers belonging to the set of interest, $m_A$ is the number of associated markers, and $m_{AF}$ is the number of associated markers belonging to the feature. Thus $m$, $m_A$, and $m_{AF}$ are fixed. 

We consider two properties of a marker; 1) it is associated with the phenotype, and 2) it belongs to the genomic feature of interest. Let $H_0$ denote the null hypothesis which states that the two properties of a marker are independent, or likewise, that the associated markers are picked at random from the total population of tested markers. \citet{Rivals2007} shows that this can be formulated and tested in a number of ways. The different tests can be evaluated using an exact (hypergeometric), approximate ($\chi^2$), or empirical distribution ($T_{Sum}$) under the null hypothesis.

### Hypergeometric test

The total number of markers that belong to the genomic feature of interest and that are associated with the phenotype can be computed as:

\begin{equation}
	T_\textrm{Count} = m_{AF} = \sum_{i=1}^{m_F} \matr{I}(t_i > t_0),
\end{equation}

where $t_i$ is the {i\th} single marker test statistics [or should it be statistic (singular)], $t_0$ is an arbitrary chosen threshold for the single marker test statistics [s or not], and $I$ is an indicator function that takes the value $1$ if the argument $(t_i > t_0)$ is satisfied. 

[EXTREMELY LONG SENTENCE - I DON'T KNOW HOW TO FIX IT] The number of associated markers that belong to a genomic feature, $m_{AF}$, can be modelled using a hypergeometric distribution that has a discrete probability distribution that describes 
the probability of $m_{AF}$ successes in $m_F$ draws without replacement (can only be drawn one time) from a finite population of size $m$ containing exactly $m_A$ successes. Thus if the null hypothesis is true (associated markers are picked at random from the total population of tested markers), then the observed value $m_{AF}$ is a realization of the random variable $M_{AF}$ 
having a hypergeometric distribution with parameters $m$, $m_A$, and $M_F$, which we denote by $M_{AF} \sim \distr{Hyper}(m, m_A, m_F)$. 

However, the hypergeometric test assumes that the markers being sampled are independent, a rather strong assumption in genetic data. Therefore, the hypergeometric test might not correctly identify significant association, but instead associated markers that are strongly correlated \citep{Goeman2007}.

### \texorpdfstring{$\chi^2$ test}{Chi-square test}
The second summary statistic is based on a $\chi^2$ test. Let the observed data be presented in a contingency table where each observation is allocated to one cell of a two-dimensional array of cells according to the values of the two outcomes:

\begin{center}
\begin{threeparttable}
\caption{Contingency table for $\chi^2$ test in two-step approach.}
	\begin{tabular}{l|cc|c}
	    \toprule
                      & In feature & Not in feature & \hspace{1em} Total \hspace{1em} \\
			\midrule
			Associated      & $m_{AF}$ & $m_{AnF}$ & $m_A$ \\
			Not associated \hspace{0.5em}
			                & $m_{nAF}$ & $m_{nAnF}$ & $m_{nA}$ \\
			\midrule
			  Total         & $m_F$ & $m_{nF}$  & $m$ \\
			\bottomrule
	\end{tabular}
	%\begin{tablenotes}[para]
	%  $m_A$: Associated; $m_NA$
	%\end{tablenotes}
\end{threeparttable}
\vspace{1em}
\end{center}

Let again $H_0$ denote the null hypothesis that the property to belong to the genomic feature of interest, and that to be associated, are independent. If the occurrence of these two outcomes are statistically independent, we expect the number in the {ij\th} cell to be $f_{ij} = \frac{m_i m_j}{m^2}$. Based on this expectation we can compute the following summary statistic:

\begin{equation}
  T_{\chi^2} = \sum_{i=1}^2 \sum_{j=1}^2 \frac{\left(m_{ij} - m \cdot f_{ij} \right)^2}{m \cdot f_{ij}}
\end{equation}	

where $f_{ij}$ is the observed frequency in the contingency table. This is called the $\chi^2$ test for independence and it has been shown that the $T_{\chi^2}$ variable is asymptotically $\chi^2$ distributed with one degree of freedom \citep{Wackerly1996,Rivals2007}. The alternative hypothesis corresponds to the variables having an association or relationship, where the structure of this relationship is not specified.


In summary, under the null hypothesis that the probability of a marker belonging to a genomic feature is independent of being associated to the trait phenotype (i.e.\ $p_{AF} = p_{nAF}$),
the exact distribution of $M_{AF}$ is the hypergeometric distribution $M_{AF} \sim \distr{Hyper}(m, m_A, m_F)$. This distribution can, if $m$ is large, be approximated with the bionomial distribution $M_{AF} \sim Bi(m_A, m_F/ m)$. If the two samples, are large, it is also possible to exhibit an approximately normal variable $Z$ or its square $D^2 = Z^2$, the latter being hence approximately $\chi^2$ distributed with one degree of freedom.

One of the differences between the hypergeometric and $\chi^2$ test statistic is that the latter implicitly distinguishes between over- or under-representation, i.e. the squared difference between the expected and observed counts for all the 4 cells contribute to the $T_{\chi^2}$ test statistic. It is possible to test for both over-representation ($p_{AF} > p_{nAF}$) or under-representation ($p_{AF} < p_{nAF}$).

Both tests are potentially of interest for understanding the genetic basis of complex traits. If the number of associated markers is very small in the genomic feature then it may be interpreted as selection/highly conserved region. If the number of associated markers is large in the genomic feature then this may indicate we have identified an important feature underlying the genomic variance of the trait. 

In cases where both over-representation and under-representation of genomic features are of interest then it is generally most appropriate to consider a two-sided test. It is also possible to define more detailed and specific hypothesis such as testing whether the associated markers contribute negatively or positively to the trait of interest.

However, there is the arbitrariness of the threshold for determining `significantly associated', 
no matter how it is chosen and markers whose test statistics differ by a tiny amount may be treated completely differently. By design this test will have high power to detect association if the genomic feature harbour markers with large effects, but it will not detect a situation where there are many markers with small to moderate effects \citep{Newton2007}. In this case, it is more powerful to use a summary statistic such as the mean or sum of the test statistics for all markers belonging to the same genomic feature. 


###  \texorpdfstring{$T_\textrm{Sum}$}{T\_Sum}
As noted above, if the phenotypic trait of interest is governed by many markers with small to moderate effects, counting `significantly associated' markers neglects a lot of information. We therefore consider the third summary statistic

\begin{equation}
  T_\textrm{Sum} = \sum_{i=1}^{m_F} t_i
\end{equation}

where $t_i$ is a test statistic for the {i\th} marker. There are number of choices for $t_i$ such as likelihood ratio, the score based statistic, or the predicted marker effects, and they might be transformed by e.g.squaring. The nature of $T_\textrm{Sum}$ is therefore difficult to describe in terms of exact or approximate distributions, and is included here as an intuitive example where empirical distributions are useful.


## Permutation versus exact and asymptotic test
If we can derive an exact distribution of test statistic under the null hypothesis then we can  use this to determine the level of statistical significance for the observed test statistic. The advantage of this is that it is computationally fast and that it works better if the sample size (i.e.\ $n$ number of observation) is small. However, many of the test statistics are derived based on an asymptotic distribution. If the sample size is small the asymptotic formula's used to calculate the p-value may not be correct. In this case a different approach could be to find the p-value using a permutation method. 

A drawback of the permutation method is that it is hard to demonstrate very low p-values.  Showing that a p-value is lower than $10^{-7}$ for example, needs at least $10^7$ permutations. Often if the sample size is small, the total number of permutations is not large enough to attain very low significance levels. 

The manner of which we permute the data is not arbitrary, but depends on the nature of the null hypothesis being tested. \citet{Goeman2007} classified the null hypotheses as either \emph{self-contained} or \emph{competitive}.

A self-contained null hypothesis assumes that the marker, or set of markers, is not associated to the phenotypic trait, or has an effect without comparison to other markers of sets. I.e. the similarity between observations and genetics is incidental. To obtain an empirical distribution of the test statistic under a self-contained null hypothesis, we can shuffle the observations thus breaking the link between observations and genetics. This can be referred to as a subject-randomisation approach \citep{Goeman2007}, but we refer to it as a `permutation' approach. However, if using models with multiple random effects, where the association between only one of the random effects is in question, shuffling the observations would break the link for all random effects, rendering the permutations useless. In this case, care should be taken to permute the link between the observations and the random effect in question.

A competitive null hypothesis assumes that the marker, or set of markers, is not \emph{more} associated than any other marker or set of markers. An empirical distribution for a competitive null hypothesis is then obtained by sampling random sets of markers. However, all parameters that might influence the test statistic must be the same. I.e. if the number of markers influence the test statistic, the same number of markers must be sampled repetitively to form the random sets. And if there is an inherent structure between the markers in the set, this structure should be present for the random sets.














