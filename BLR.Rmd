---
title: "Bayesian Linear Regression Models"
author: "Palle Duun Rohde, Izel Fourie Sørensen & Peter Sørensen"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    citation_package: natbib
    includes:
      in_header: preamble.tex
  html_document:
    number_sections: yes
    includes:
      in_header: mathjax_header.html
  word_document: default
bibliography: [qg2021.bib, book.bib, packages.bib]
link-citations: yes
---

# Introduction

Bayesian linear regression models have been proposed as a unified framework for gene mapping, prediction of genetic predispostion, estimation of genetic parameters and effect size distribution ([Moser et al. 2015](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4388571/)). 

Bayesian linear regression models attempts to account for the underlying genetic architecture of the trait. This is achieved by using many linked markers covering the entire genome to jointly estimate marker effects, and by allowing the genetic signal to be heterogeneous distributed over the genome (i.e. some regions have stronger genetic signal than others). This may in some situations allow a more accurate estimate of the true underlying genetic signal leading to more accurate predictions. 

Bayesian linear regression models can also be used to map genetic variants associated with phenotypes and to estimate the total variance explained by the genetic markers. Because they fit all markers simultaneously and account for linkage disequilibrium between markers, they should have greater power to detect true associations, find less false negatives and give unbiased estimates of the larger marker effects. They can also provide information about the genetic architecture of the trait from the hyper-parameters of the distribution of marker effects.

Bayesian linear regression models fit all markers simultaneously and their effects as drawn from a prior distribution that attempts to match the true distribution of marker effects as closely as possible. However the true distribution of effect sizes is unknown but a mixture of normal distributions can approximate a wide variety of distributions by varying the mixing proportions. Erbe et al. used this prior and included one component of the mixture with zero variance. A similar model was proposed by Zhou et al. but with a mixture of two normal distributions, one with a small variance and one with a larger variance.

In the following the statistical model, prior distributions of model parameters, algorithms for estimation of model parameters, extensions for handling multiple marker sets and multiple traits for the Bayesian linear regression models is presented. We will present two alternative BLR models, BLR model for marker effects and BLR model for individual effects.  


# Bayesion Linear Regression Model for Marker Effects

## Statistical model and model parameters
The linear model contains the observation vector for the trait(s) of interest ($y$), the `fixed effects` that explain systematic differences in $y$, and the `random genetic effects` $a$ and random residual effects $e$.

A matrix formulation of a general model equation is:
\begin{align}
y &= Xb + a + e \notag
\end{align}

where
\begin{align}
y &: \text{is the vector of observed values of the trait,} \notag \\
b &: \text{is a vector of fixed effects,} \notag \\
a &: \text{is a vector of random genetic effects,} \notag \\
e &: \text{is a vector of random residual effects,} \notag \\
X &: \text{is a known design matrix that relates the elements of b to their corresponding element in y.} \notag 
\end{align}

The phenotype is related to the set of genetic markers under the multiple linear regression model:

\begin{align}
\bf{y} = \bf{Wb} + \bf{e}
\end{align}

where $\bf{y}$ is the phenotype, $\bf{W}$ a matrix of SNP genotypes, where values are standardised to give the ijth element as: $w_{ij} = \left( {x_{ij} - 2p_j} \right){\mathrm{/}}\sqrt {2p_j\left( {1 - p_j} \right)}$, with $x_{ij}$ the number of minor (or alternative) alleles (0, 1 or 2) for the ith individual at the jth SNP and $p_j$ the minor (or alternative) allele frequency. $\bf{b}$ are the genetic effects for each SNP, and $\varepsilon$ the residual error. The dimensions of $\bf{y}$, $\bf{W}$, $\bf{b}$ and $\bf{e}$ are dependent upon the number of traits, $k$, the number of SNP markers, $m$, and the number of individuals, $n$, and are described in the sections below. 

The residuals, ${\bf{e}}$, are a priori assumed to be independently and identically distributed multivariate normal with null mean and covariance matrix ${\bf{I}\sigma}_{\epsilon}^2$. 


## Prior distributions of model parameters

Most complex traits and diseases are likely highly polygenic, with hundreds to thousands of causal variants, most frequently of small effect. So, the prior distribution must include many small and few large effects. Furthermore marker effects are a priory assumed to be uncorrelated (but markers can be in strong linkage disequilibrium and therefore a high posterior correlation). Many priors for marker effects have been proposed. These priors come more from practical (ease of computation) than from biological reasons. Each prior originates a method or family of methods, and we will describe them next, as well as their implications.

1. Normal distribution: Random regression BLUP (RR-BLUP), SNP-BLUP, G-BLUP
2. Normal distribution with unknown variances: BayesCPi0, GREML
3. Student (t) distribution : BayesA
4. Mixture of Student (t) distribution and spike at 0: BayesB
5. Mixture of Normal distributions and spike at 0: BayesCPi and BayesR
6. Double exponential: Bayesian Lasso

### Prior marker variance SNP-BLUP (RR-BLUP and G-BLUP) {.unlisted .unnumbered}

In SNP-BLUP the prior for each marker effect follows a priori a normal distribution with a variance $\sigma_b^2$ which is constant across markers:

\begin{align}
p(b)=\prod_ip(b_i)
\end{align}

where

\begin{align}
p(b_i) = N\left(0, \sigma_b^2 \right)
\end{align}

In a normal distribution most effects are concentrated around 0, whereas few effects will be large. Therefore, the prior assumption of normality precludes few markers of having very large effects – unless there is a lot of information to compensate for this prior information. 

The allele frequencies should be the ones in the population where the genetic variance was estimated (e.g., the base population of the pedigree) and not the current, observed populations. 

We assume that markers are independent one from each other. This can be equivalently written
as:

\begin{align}
p(b)=MVN(0,D)
\end{align} 

\begin{align} 
Var(b)=D=I\sigma_b^2
\end{align} 

where MVN stands for multivariate normal. This formulation including D will be used again
throughout these notes.


### Prior marker variance
Marker “variances”, can, however, be included within a Bayesian framework. The Bayesian framework will postulate a non-normal distribution for marker effects, and this non-normal distribution can be explained as a two-stages (or hierarchical) distribution. 
In the first stage, we postulate that each marker has a priori a different variance from each other:
\begin{align}
p(b_i|\sigma_{{b}_i}^2) = N\left(0, \sigma_{{b}_i}^2 \right)
\end{align}
In the second stage, we postulate a prior distribution for the variance themselves:
\begin{align}
p(\sigma_{{b}_i}^2 | something) = p\left(..... \right)
\end{align}
This prior distribution helps (the estimate of $\sigma_{{b}_i}^2$ is more accurate, in the sense of lower mean square error) although it will still be far from reality (e.g. (Gianola et al. 2009)). This is very convenient because the solving algorithm simplifies greatly and most Bayesian regression models are based in this idea.

### Prior marker variance Bayes A {.unlisted .unnumbered}

The simplest idea is to assume that a priori we have some information on the marker variance. For instance, this can be $\sigma_b^2$. Thus, we may attach some importance to this value and use it as prior information for $\sigma_{{b}_i}^2$. A natural way of doing this is using an inverted chi-squared distribution with with  $\upsilon_{b}$ degrees of freedom and scale parameter  $S_{b}^{2}=\upsilon_{b}\sigma_b^2$ 

\begin{align}
p(b_i|\sigma_{{b}_i}^2) = N\left(0, \sigma_{{b}_i}^2 \right)
\end{align}

In the second stage, we postulate a prior distribution for the variance themselves:

\begin{align}
p(\sigma_{{b}_i}^2 |\upsilon_{b},S_{b}^{2}) = S_{b}^{2}\chi_{\upsilon_{b}}^{-1}
\end{align}

The value of $\sigma_b^2$ should be set as 

$\sigma_b^2=\frac{\upsilon_{b}-2}{\upsilon_{b}}\frac{\sigma_g^2}{2\sum_{i}p_i(1-p_i)}$ because the variance of a t distribution is $\frac{\upsilon_{b}}{\upsilon_{b}-2}$. It can be shown
that this corresponds to a prior on the marker effects corresponding to a scaled t distribution (Gianola et al. 2009):

\begin{align}
p(b_i|\sigma_{b}^2,\upsilon_{b}) = \sigma_{b}t\left(0, \upsilon_{b} \right)
\end{align}

which has the property of having “fat tails”. This means that large marker effects are more likely a priori compared to a normal distribution.

### Prior marker variance Bayes B {.unlisted .unnumbered}

A very common thought was that there were not many causal loci. So a natural thinking is to consider that many markers do not have effect because they cannot trace causal loci. This originated the method known as BayesB, that simply states that the individual marker variance, $\sigma_{{b}_i}^2$, is potentially zero. Note that this cannot happen for BayesA as the a priori chi-squared distribution prevents any marker variance from being zero. This idea corresponds to a more complex prior as follows:

\begin{align}
p(b_i|\sigma_{{b}_i}^2) = N\left(0, \sigma_{{b}_i}^2 \right)
\end{align}



\begin{align}
p(\sigma_{{b}_i}^2|S_{b}^{2},\upsilon_{b}) = \left\{ {\begin{array}{*{20}{l}} 0 \hfill & {{\mathrm{with}}\,{\mathrm{probability}}\,\pi,} \hfill \\ {S_{b}^{2}\chi_{\upsilon_{b}}^{-1}} \hfill & {{\mathrm{with}}\,{\mathrm{probability}}\,1-\pi,}  \end{array}} \right.
\end{align}

then when $\sigma_{{b}_i}^2=0$ it follows that $b_i=0$. BayesB has a complex algorithm because it does involve the computation of a complex likelihood and experience suggest that this methods is not very robust.


### Prior marker variance Bayes C {.unlisted .unnumbered}

Habier et al. 2011 suggests the possibility of a simpler prior scheme where markers having an effect would be assigned a “common” variance such as $\sigma_{b}^2$. This is simpler to be explained by introducing additional variables $\delta_i$ which explain if the i-th marker has an effect or not. In turn, these variables $\delta$ have a prior distribution called Bernouilli with a probability $\pi$ of being 0. Therefore the hierarchy of priors is:

\begin{align}
p(\beta_j|\delta_i,\sigma_{{b}_i}^2,\pi) = \left\{ {\begin{array}{*{20}{l}} 0 \hfill & {{\mathrm{with}}\,{\mathrm{probability}}\,\pi,} \hfill \\ {\sim N(0,\sigma_{{b}_i}^2)} \hfill & {{\mathrm{with}}\,{\mathrm{probability}}\,1-\pi,}  \end{array}} \right.
\end{align}

\begin{align}
p(\sigma_{{b}_i}^2 |\upsilon_{b},S_{b}^{2}) = S_{b}^{2}\chi_{\upsilon_{b}}^{-1}
\end{align}

where $S_{b}^{2}=\sigma_b^2\upsilon_{b}$ with

$\sigma_b^2=\frac{\sigma_g^2}{(1-\pi)2\sum_{i}p_i(1-p_i)}$ because the variance of a t distribution is $\frac{\upsilon_{b}}{\upsilon_{b}-2}$. Experience shows that this prior hierarchy is more robust than BayesB (not shown), the reason being that, at the end (after fitting the data), the values of $\sigma_{{b}_i}^2$ are little dependent on the prior. Thus the model may be correct even if the prior is wrong.


### Prior marker variance Bayesian Lasso {.unlisted .unnumbered}

The Bayesian Lasso (Park and Casella 2008 ; Campos et al. 2009 ; A. Legarra, Robert-Granié, Croiseau, et al. 2011) suggests a different way to model the effect of markers. Instead of setting a priori some of them to 0, it sets them to very small values which corresponds to the following a priori distribution of markers:

\begin{align}
p(b_i|\sigma_{{b}_i}^2) = N\left(0, \sigma_{{b}_i}^2 \right)
\end{align}


In the second stage, we postulate a prior distribution for the variance themselves:

\begin{align}
p(\sigma_{{b}_i}^2 |\lambda) = \frac{\lambda^2}{2}exp(-\frac{\lambda^2}{2}\frac{\sigma_{{b}_i}^2}{\sigma_e^2})
\end{align}

In this case, a natural way of fitting the prior value of $\lambda$ is as (Pérez et al. 2010)

\begin{align}
\frac{2}{\lambda^2}=\frac{\sigma_g^2}{\sigma_e^22\sum_{i}p_i(1-p_i)}
\end{align}

In this case, $\lambda$ can be thought of as a ratio between marker variance and residual variance (signal-to-noise). 

We can use any coding (centered, 101 or 012 or 210) in W for Bayesian methods. The estimated $\hat g$ will be the same, the estimated $\sigma_b^2$ or $\pi$ will be the same, and the estimated genetic variance computed using, for instance, $\sigma_g^2=\sigma_b^22\sum_{i}p_i(1-p_i)$, will be the same too. These results are convenient because they assure us that any allele coding is convenient. However, this result does not apply to the all features. For instance, the standard deviation (and therefore, in animal breeding words, the “model-based” reliability) of estimated genetic values $\hat g$ is not
invariant to parameterization, because there will be a part of the overall mean absorbed, or not, by $W\hat b$. This implies that reports of the posterior variance of $\hat g$ will depend on the allele coding.


### Prior marker variance Bayes R {.unlisted .unnumbered}

The marker effects, b, are a priori assumed to be sampled from a mixture with a point mass at zero and univariate normal distributions conditional on common marker effect variance $\sigma_{\beta}^2$,and variance scaling factors, ${\bf{\gamma}}$:

\begin{align}
\beta _j|\pi ,\sigma _\beta ^2 = \left\{ {\begin{array}{*{20}{l}} 0 \hfill & {{\mathrm{with}}\,{\mathrm{probability}}\,\pi _1,} \hfill \\ {\sim N(0,\gamma _2\sigma _\beta ^2)} \hfill & {{\mathrm{with}}\,{\mathrm{probability}}\,\pi _2,} \hfill \\ \vdots \hfill & {} \hfill \\ {\sim N(0,\gamma _C\sigma _\beta ^2)} \hfill & {{\mathrm{with}}\,{\mathrm{probability}}\,1 - \mathop {\sum}\nolimits_{c = 1}^{C - 1} {\pi _c,} } \hfill \end{array}} \right.
\end{align}

where  $\pi = \left(\pi _{1}, \pi _{2}, ...., \pi _{C} \right)$  is a vector prior probabilities and  $\gamma = \left(  \gamma _{1}, \gamma _{2}, ....., \gamma _{C} \right)$ is a vector of variance scaling factors for each of C marker variance classes. The $~\gamma$ coefficients are prespecified and constrain how the common marker effect variance $\sigma _{\beta }^{2}$ scales within each mixture distribution. Typically $\gamma = \left( 0,0.01,0.1,1.0 \right)$. and  $\pi=\left(0.95,0.02,0.02,0.01\right)$. 


The prior distribution for the marker variance $\sigma_{\beta}^{2}$  is assumed to be an inverse Chi-square prior distribution,$\chi^{-1}\left(S_{\beta},\nu_{\beta}\right)$.

The proportion of markers in each mixture class $\pi$ follows a Direchlet $(C,c+\alpha)$ distribution, where c is a vector of length C that contains the counts of the number of variants in each variance class and $\alpha=(1,1,1,1)'$.   

Using the concept of data augmentation, an indicator variable ${\bf{d}}=(d_1,d_2,..,d_{m-1},d_m)$, is introduced, where $d_j$ indicates whether the j’th marker effect is zero or nonzero.


### Marker variance {.unlisted .unnumbered}

The total genetic variance explained by markers can be expressed as (assuming linkage equilibrium): 

\begin{align}
\sigma_g^2 = 2\sum_{i} p_i(1-p_i){\hat{b}}_i^2
\end{align}

In most cases we do not know the marker effects, but may have some information prior variance then we can substitute the term ${\hat{b}}_i^2$ by it expectation which is $\sigma_{{b}_i}^2$ and therefore

\begin{align}
\sigma_g^2 = 2\sum_{i} p_i(1-p_i)\sigma_{{b}_i}^2
\end{align}

and if we assume that all markers have the same variance a priori then   

\begin{align}
\sigma_{{b}_i}^2 = \frac{\sigma_g^2}{2\sum_{i} p_i(1-p_i)}
\end{align}

which puts the prior variance of the markers as a function of the genetic variance of the population. This result is widely used in many applications in genomic prediction.


\begin{comment}
## Estimation of marker variance
Often, estimates of variance components from field data are unreliable, too old, or not directly available. In this case, it is simpler to estimate those variances from marker data. Although this is typically done using GREML, it can also be done in marker models. This was the case of (A.Legarra, Robert-Granié, Manfredi, et al. 2008) in mice, and it has later been used to estimate genetic variances in wild populations (Sillanpaa 2011). It is very simple to do using Bayesian inference, and posterior estimates of the variances $\sigma_b^2$ and $\sigma_e^2$ are obtained. This method has been described by (Habier et al. 2011) as BayesC with $\pi=0$ and that is how we will cite it. The algorithm is fairly simple from a GSRU iteration scheme. Instead of iterating the solution, we sample it, 

The algorithm requires initial values of variances and also prior information for them. Typical prior distributions for variance components are inverted-chi squared ($\chi^-2$) scaled by constants ( $S_{b}^{2},S_{e}^{2}$ for marker and residual variances) with some degrees of freedom ($\upsilon_{b}, \upsilon_{e}$). The degrees of freedom represent the amount of information put on those variances and therefore wheras 4 is a small value (and almost “irrelevant”) 10,000 is a very strong prior. Typical values used in practice can be 4, for instance. 

On expectation, if we use a priori $S_{e}^{2}$ and $\upsilon_{e}$ then $E(\sigma_e^2|S_{e}^{2},\upsilon_{e})=S_{e}^{2}/\upsilon_{e}$ One may use previous estimates and put therefore

$S_{e}^{2}=\sigma_e^2\upsilon_{e}$
and
$S_{b}^{2}=\sigma_b^2\upsilon_{b}$


$\sigma_b^2=\frac{\sigma_g^2}{2\sum_{i}p_i(1-p_i)}$

Please note that in other parameterizations $E(\sigma_e^2|S_{e}^{2},\upsilon_{e})=S_{e}^{2}$ and $E(\sigma_b^2|S_{b}^{2},\upsilon_{b})=S_{b}^{2}$ and therefore the Scale factor is in the same scale as the regular variances, and we can use $S_{e}^{2}=\sigma_e^2$ and $S_{b}^{2}=\sigma_b^2$

## Effect of allele coding on BLR

## Reliabilities from BLR
Daniel/Gustavo BLUP notes + Legarra paragraph

\end{comment}

## Estimation of model parameters

Bayesian linear regression methods use an iterative algorithm for estimating joint marker effects. Estimation of the joint marker effects depend on additional model parameters such as a probability of being causal ($\pi$), an overall marker variance ($\sigma_{\beta}^2$), and residual variance ($\sigma_e^2$). Estimation of model parameters can be done using MCMC techniques by sampling from fully conditional posterior distributions or by using an EM based approximation (emBayesR) as a fast counterpart to a MCMC version of BayesR ([Wang et al. 2015](https://gsejournal.biomedcentral.com/articles/10.1186/s12711-014-0082-4#MOESM2)). 

### An MCMC algorithm for BayesR (mcBayesR)

An MCMC algorithm can be used to obtain estimates of parameters in BayesR models.
The multiple linear regression model is parameterized in terms of  $\theta= (  \sigma_{e}^{2}, \sigma_{ \beta}^{2}, \gamma , \pi ,~b,~d)$. The full conditional sampling distributions for these parameters are presented below. 

The joint posterior for all parameter in the multiple regression model can be written as :

\begin{align}
 f \left(  \sigma _{e}^{2}, \sigma _{ \beta }^{2}, \gamma , \pi ,~b,~d \vert y \right) \propto \left( y \vert  \sigma _{e}^{2}, \sigma _{ \beta }^{2}, \gamma , \pi ,~b,~d \right) f \left( b \vert d, \gamma , \sigma _{ \beta }^{2} \right) f \left( d \vert  \pi  \right) f \left(  \sigma _{e}^{2} \right) f \left(  \sigma _{ \beta }^{2} \right) f \left(  \pi  \right)
\end{align}

The parameters  $d_{j}$  and  $b_{j}$  are sampled jointly from their joint full conditional distributions, which can be written as the product of the full conditional distribution of  $b_{j}$  given  $d_{j}$  and the marginal full conditional distribution of  $d_{j}$: 

\begin{align}
f_{j} \left( d_{j},b_{j} \vert  \theta _{-j},\widetilde{y} \right) =f_{j} \left( b_{j} \vert  \theta _{-j},d_{j},\widetilde{y} \right) f_{j} \left( d_{j}, \vert  \theta _{-j},\widetilde{y} \right)
\end{align}

Where $\theta _{-j}$ is all parameters except $d_{j}$  and $b_{j}$, and $\widetilde{y}=y-X_{-j}b_{-j}$ is the phenotype adjusted for all marker effects except for j'th marker ($b_{j}$). 


The full conditional distribution for \( b_{j}\) can be be written as:
\begin{align}
f_{j} \left( b_{j} \vert  \theta _{-j},d_{j},\widetilde{y} \right) \propto N \left( C_{j}^{-1}r_{j},C_{j}^{-1} \right) 
\end{align}

where   $C_{j}^{-1}=\frac{\sigma_{e}^{2}}{x_{j}^{'}x_{j}+\frac{\sigma_{e}^{2}}{\sigma_{c}^{2}}}$ and $r_{j}=\frac{x_{j}^{'}\widetilde{y}}{\sigma _{e}^{2}}$ 


The marginal full conditional probability for the indicator variable of  $d_{j}$ is:
\begin{align}
f_{j} \left( d_{j}=c \vert  \theta _{-j},\widetilde{y} \right) =\frac{f_{j} \left( \widetilde{y} \vert d_{j}=c, \theta _{-j} \right) f \left( d_{j} \vert  \pi _{c} \right) }{ \sum _{k=1}^{C}f_{j} \left( \widetilde{y} \vert d_{j}=k, \theta _{-j} \right) f \left( d_{j} \vert  \pi _{k} \right) } 
\end{align}

where $f_{j} \left( \widetilde{y} \vert d_{j}=c, \theta _{-j} \right) = \left(  \sigma _{c}^{2} \right) ^{-0.5} \left( C_{j}^{-1} \right) ^{0.5}\exp  \left[ \frac{1}{2}C_{j}^{-1}r_{j}^{2} \right]$ 

From these expression probabilities for the categorical distribution for an arbitrary number of mixture components can be computed. Given these probabilities sample from a categorical distribution, which determines which class the variant will be sampled from. Conditional on the marker variance class sample the effect from the relevant normal distribution or give it a zero effect. To sample from the categorical distribution: 1) create a vector of cumulative probabilities calculated from above ordered by category, and 2) accept the lowest c such that the cumulative probability > u, where u is sample from a uniform distribution $U\left(0,1\right)$.

The proportion of markers in each mixture class $\pi$ follows a $Direchlet~\left( C,c + \alpha\right)$  distribution, where $\textbf{c}$ is a vector of length C that contains the counts of the number of variants in each variance class and $\alpha = \left( 1,1,..,1 \right)$.

The full conditional posterior distribution for  $\sigma _{ \beta }^{2}~$ is a scaled inverse chi-squared distribution with  $\upsilon _{ \beta }=\widetilde{ \nu }_{ \beta }+q$  degrees of freedom and scale parameter $S_{ \beta }^{2}=\frac{\widetilde{ \nu }_{ \beta }\widetilde{S}_{ \beta }^{2}+ \sum_{j=1}^{q}\frac{b_{j}^{2}}{ \gamma _{d_{j}}}}{\widetilde{ \nu }_{ \beta }+q}$  where $\widetilde{ \nu }_{ \beta }~$ and  $\widetilde{S}_{ \beta }^{2}$ are the prior degrees of freedom and scale parameters.

The full conditional posterior distribution for $\sigma_{e}^{2}$ is a scaled inverse chi-squared distribution with $\upsilon_{e}=\widetilde{\nu}_{e}+n$ degrees of freedom and scale parameter $S_{\beta}^{2}=\frac{\widetilde{\nu}_{e}\widetilde{S}_{e}^{2}+sse}{\widetilde{\nu }_{e}+n}$  where $\widetilde{\nu}_{e}$ and $\widetilde{S}_{e}^{2}$ are the prior degrees of freedom and scale parameters and $sse=y^{'}y-b^{'}r^{\ast}-b^{'}X^{'}y$.


## Mapping of variants

The value of $1-\pi$ (the number of markers having an effect) can be either fixed to a value or estimated from data. This is achieved in the last lines of the code above. How is this possible? Intuitively, we look at the number of markers who have ($\delta_i=1$) or not ($\delta_i=0$) an effect. Then we add a prior information on $\pi$. This comes in the form of a Beta(a, b) distribution, which is a distribution of fractions between 0 and 1, saying that our fraction is a priori 'like if' we had drawn a black balls and b red balls from an urn to make $\pi= \frac{a}{a+b}$. The total genetic variance explained by markers in the Bayes C model is equal to

\begin{align}
\sigma_g^2=\sigma_b^2(1-\pi)2\sum_{i}p_i(1-p_i)
\end{align}

Thus, the same total genetic variance can be achieved with large values of $\sigma_b^2$ and small values of $(1-\pi)$ or the opposite. This implies that there is a confusion between both, and it is not easy to find out how many markers should be in the model.
How can we declare significance? There is no such thing as p-values. We may though use the Bayes Factor (Wakefield 2009 ; Varona 2010) :

\begin{align}
BF_i=\frac{1-\pi}{\pi}\frac{p(\delta_i=1|y)}{1-p(\delta_i=1|y)}
\end{align}

What thresholds should we use for BF? We can use a scale adapted by (Kass and Raftery 1995) sometimes used in QTL detection (Varona,García-Cortés, and Pérez-Enciso 2001 ; Vidal et al. 2005):
BF= 3-20 'suggestive'
BF= 20-150 'strong'
BF>150 'very strong'

Something remarkable is that there is no need for multiple testing (Bonferroni) correction because all SNP were introduced at the same time, and the prior already penalizes their estimates (Wakefield 2009). 


## Extensions to Multiple Traits and Multiple Components

Bayesian linear regression models have been extended to multiple trait analyses. A multiple trait analysis is a natural choice in quests for understanding and dissecting genetic correlations between traits using genetic markers, e.g., evaluating whether pleiotropy or linkage disequilibrium are at the root of between-trait associations. Furthermore pleiotropy among traits can be utilised to increase accuracy of genomic predictions. 

Multiple trait Bayesian linear regression models can implemented under an restrictive assumption that a locus simultaneously affects all the traits or none of them. However this assumption is not biologically meaningful, especially in multiple trait analyses involving many traits. This assumption of genetic architecture is violated if some loci have no effect on at least one of the traits while having an effect on the remaining traits.   

Therefore a general multiple trait Bayesian regression model allowing a broad range of mixture priors have been proposed. In this model a locus is allowed to affect any combination of traits, e.g., in a 2-trait analysis, the “restrictive” model only allows two situations, whereas in the general model allow all $2^2=4$ situations. This model is particularly interesting because it provides insight into whether markers affect all, some, or none of the traits addressed.For example, the proportion of markers in each of the (0,0), (1,0), (0,1) and (1,1) categories, where (0,0) means “no effect,” and (1,1) denotes “effect” on both traits. 

A further extensions Bayesian regression model is to partioning the markers into multiple sets based on prior knowledge on genomic features.


### Prior distributions for multiple traits
In practice, genetic variances from previous conventional analyses are usually used to construct priors for marker effect variances. For single trait analyses, under some assumptions, it can be shown that the marker effect variance XX can be obtained as XX (2) where XX is the genetic variance, XX is the allele frequency for locus j, and $\pi$ is the probability that a marker has a null effect (Habier et al. 2007; Gianola et al. 2009; Fernando and Garrick 2013). Following a similar strategy, the marker effect covariance matrix XX in a two-trait analysis can be obtained as XX (3) where XX is the genetic covariance matrix and XX XX and XX are the probabilities a marker has null effects on the first trait but not the second trait, on the second trait but not the first trait, or on neither trait. Thus the probability that a marker has an effect on the first trait can be obtained as XX which is the denominator of the upper left element in (3). This strategy relating marker effect covariance matrix to genetic covariance matrix can be readily extended to >2 traits. Note that positive definite matrix XX may result in negative definite matrix XX using (3), especially when the prior for the probability a marker has null effects is far from the real value. In that case, the diagonal elements of XX which are the marker effect variances for different traits, can be obtained using (2), where $\pi$ may be estimated from previous single-trait analyses, and the off-diagonal elements of XX may be set to zero to guarantee positive definiteness of XX


### Multi-trait variable selection
In regard to a single trait, a locus either has an effect, or it does not. Hence, the scalar parameter $\pi$ (and its complement XX) completely defines this circumstance. In a multi-trait setting, it is conceivable that loci that influence one trait, may or may not influence other traits. In that circumstance, a vector XX is required to define the genetic architecture. The number of parameters that constitute the vector XX is XX which grows rapidly with the number of traits. In most cases, the researcher will have little or no knowledge of the likely extent of pleiotropy of loci that influence two traits, other than knowing or having an estimate of the genetic covariance. There are two simple ways to reduce this complexity in priors.

First, one can assume, as did Jia and Jannink (2012), that, in the context of variable selection, a locus should be selected for all of the traits or selected for none of the traits, reducing the required probabilities to being analogous to the single trait $\pi$ and XX This approach has the advantage of simplicity, but the disadvantage that many effects might need to be estimated for loci that have no effect on a trait, and this may erode the accuracy of prediction. This should not be a problem for asymptotically large datasets, as in that case the fitted locus effects should converge to zero for those traits not influenced by that locus.

A second simple way to accommodate the multiple trait circumstance is to assume the XX parameters can be derived from t trait-specific parameters. However, when the probability that a single trait locus has an effect is small for each of two or more traits, the pair-wise probability that a locus affects all the traits will be the product of those small probabilities, making it very difficult for loci to enter the model for all traits simultaneously.

The better way to solve this problem is to use a hyper-parameter XX that completely defines the alternative models that are required to capture all the alternative forms of genetic architecture. We have shown here how this can be done, with two alternative Gibbs sampling strategies. One involves single-site sampling for one locus and trait at a time. The other samples all the alternative combinations of effects for one locus considering all traits simultaneously. We have shown that both are practical with real data and can result in improved accuracies of prediction in certain circumstances in terms of genetic architecture and size of dataset.

### Posterior sampling distributions for multiple traits

\begin{align}
b= \left[ \begin{matrix}
b_{1}\\
b_{2}\\
\end{matrix}
 \right] = \left(  \left[ \begin{matrix}
X_{1}^{'}X_{1}^{}  &  0\\
0  &  X_{2}^{'}X_{2}^{}\\
\end{matrix}
\right] ~ +I\otimes G^{-1}E \right) ^{-1} \left[ \begin{matrix}
X_{1}^{'}y_{1}^{}\\
X_{2}^{'}y_{2}^{}\\
\end{matrix}
\right] 
\end{align}

\begin{align}
G= \left[ \begin{matrix}
\sigma _{s_{1}}^{2}  &   \sigma _{s_{12}}^{2}\\
\sigma _{s_{21}}^{2}  &   \sigma _{s_{2}}^{2}\\
\end{matrix}
\right]
\end{align}

 
\begin{align}
E= \left[ \begin{matrix}
\sigma _{e_{1}}^{2}  &   \sigma _{e_{12}}^{2}\\
\sigma _{e_{21}}^{2}  &   \sigma _{e_{2}}^{2}\\
\end{matrix}
\right] 
\end{align}
 
$\sigma _{e_{12}}^{2}= \sigma _{e_{21}}^{2}=0$
 

The covariance matrix $\textbf{G}$ for the marker effect is a priori assumed to follow an inverse Wishart distribution $IW(\widetilde{S}_{\beta}^{2},\widetilde{\nu}_{\beta })$ where $\widetilde{\nu}_{\beta}$ and  $\widetilde{S}_{\beta }^{2}$ (t by t matrix) are the prior degrees of freedom and scale parameters. 

The full conditional distribution for $\textbf{G}$ the covariance matrix for the jth marker is also an inverse Wishart distribution of the form:

$IW(\widetilde{S}_{\beta}^{2} + bb' ,\widetilde{\nu}_{\beta}+m)$, where $\textbf{b}$ is a matrix (m x t) of sampled marker effects

or

$IW(\widetilde{S}_{\beta}^{2} + b_{j}b_{j}^{'}, \widetilde{\nu}_{\beta}+1)$, where  $b_{j}$ is a vector (t x 1) of sampled marker effects.

Difference between the two sampling distributions is that the former is the same for all markers whereas the latter is more flexible as it allows the markers to their own covariance matrix.

The covariance matrix $\textbf{E}$ for the residual effects is a priori assumed to follow an inverse Wishart distribution $IW(\widetilde{S}_{e}^{2}, \widetilde{\nu}_{e})$ where  $\widetilde{\nu}_{e}$ and $\widetilde{S}_{e}^{2}$ (t by t matrix) are the prior degrees of freedom and scale parameters.

The full conditional distributions for the covariance matrix \textbf{E} for residuals is also an inverse Wishart distribution of the form:

IW($\widetilde{S}_{e}^{2} + ee'$ , $~\widetilde{ \nu }_{e}+m$), where \textbf{e} is a matrix (n x t) of residual effects

The full conditional distribution for  $b_{j}~$ can be be written as:

\begin{align}
f_{j} \left( b_{j} \vert  \theta _{-j},d_{j},\widetilde{y} \right) \propto N \left( C_{j}^{-1}r_{j},C_{j}^{-1} \right)   
\end{align}

where $C=D_{j}^{'}E^{-1}D_{j}x_{j}^{'}x_{j}+G^{-1}$  and  $r_{j}=x_{j}^{'}\widetilde{y}E^{-1}D_{j}=\widetilde{b}_{j}x_{j}^{'}x_{j}E^{-1}D_{j}$ 

where  $D_{j}$  is a diagonal matrix whose kth diagonal entry is an indicator variable indicating whether the marker jth effect for trait k is zero or nonzero.

The marginal full conditional probability for the indicator variable of  $d_{j}$  is:

\begin{align}
f_{j} \left( d_{j}=c \vert  \theta _{-j},\widetilde{y} \right) =\frac{f_{j} \left( \widetilde{y} \vert d_{j}=c, \theta _{-j} \right) f \left( d_{j} \vert  \pi _{c} \right) }{ \sum _{k=1}^{C}f_{j} \left( \widetilde{y} \vert d_{j}=k, \theta _{-j} \right) f \left( d_{j} \vert  \pi _{k} \right) }
\end{align}

where  $f_{j} \left( \widetilde{y} \vert d_{j}=c, \theta _{-j} \right) = \vert C_{j}^{-1} \vert ^{0.5}\exp  \left[ \frac{1}{2}r_{j}^{'}C_{j}^{-1}r_{j}^{} \right]$

In the most general case, any marker effect might be zero for any possible combination of t traits resulting in $2^t$ possible combinations of $d_{j}$: For example, in a t=2 trait model, there are $2^t=4$  combinations for $d_{j}$: (0,0), (0,1), (1,0), (1,1).  Suppose, in general, we use numerical labels $1$,$2$,...,$l$  for the $2^t$ possible outcomes for $d_{j}$  then the prior for $d_{j}$ is a categorical distribution.


## Extensions to Summary Statistics

The multiple regression model described above can be related to the estimates of the regression coefficients from m simple linear regressions  $b_{marg}$ from a standard GWAS, by multiplying by $D^{-1}X'$  where  $D=~diag \left(x_{1}^{'}x_{1},x_{2}^{'}x_{2},...,x_{m}^{'}x_{m} \right)$  such that:

\begin{align}
D^{-1}X'y=D^{-1}X'Xb+D^{-1}X'e
\end{align}

Noting that  $b_{marg}=D^{-1}X'y$  is a vector (m $\times$ 1) of least-squares marginal regression effect estimates and the LD correlation matrix between all genetic markers  $B=D^{-0.5}X'XD^{-0.5}$. The key parameter of interest in the multiple regression model are the marker effects. These can be obtained by solving an equation system similar to:

\begin{align}
\hat{b}&= \left( X'X~ +I\frac{ \sigma _{e}^{2}}{ \sigma _{ \beta }^{2}} \right) ^{-1}X'y
\end{align}

\begin{align}
\hat{b}&= \left( X'X~ +I \lambda  \right) ^{-1}X'y
\end{align}

\begin{align}
\lambda &= \left( \frac{ \sigma _{e}^{2}}{ \sigma _{b_{1}}^{2}},\frac{ \sigma _{e}^{2}}{ \sigma _{b_{2}}^{2}},...,\frac{ \sigma _{e}^{2}}{ \sigma _{b_{m}}^{2}} \right)
\end{align}

$\sigma _{b_{j}}^{2}$ can take values $\gamma _{c} \sigma _{ \beta  }^{2}$ where $~\gamma = \left(  \gamma _{1}, \gamma _{2}, \gamma _{3}, \gamma _{4} \right)$  depending on the state of the indicator variable $d$.

In order to solve this equation system individual level data (genotypes $X$ and phenotypes $y$) is required. If these are not available, it is possible to reconstruct $X^{'}y$  and $X'X$  from summary statistics and LD reference panel. $X'X$ is derived from an LD matrix $\bf{B}$ (from population matched reference) and summary statistics: 

\begin{align}
X^{'}X=D^{0.5}BD^{0.5}
\end{align}

where  \(D_{i}=\frac{1}{\hat{\sigma}_{b_{i}}^{2}+\hat{b}_{i}^{2}/n_{i}}\) if the genotypes have been centered to mean 0 \(D_{i}=n_{i}\) if the genotypes have been centered to mean 0 and scaled to unit variance.  \( X'y\)  is derived from univariate marker effects ( \(b_{marg}\) ):

\begin{align}
 b_{marg}&=D^{-1}X'y 
 X^{'}y&=Db_{marg} 
\end{align}


### Reference LD matrix construction {.unlisted .unnumbered}

The summary statistics methods used require the construction of a reference LD correlation matrix. Typically this is done through the use of a fixed 1–10-Mb window approach, as in GCTA-SBLUP or LDpred, which sets LD correlation values outside this window to zero. Zhu and Stephens detail the reasons for using the shrinkage estimator of the LD matrix, which shrinks the off-diagonal entries of the LD correlation matrix towards zero and is required for the implementation based on summary statistics. Experimentation with different types of sparse LD correlation matrices led to the conclusion that the shrinkage estimator was the most stable for SBayesR implementation. Briefly, each element of the reference LD correlation matrix Bij is shrunk by the factor $exp(-\varrho_{ij}/2m)$, where m is taken to be the sample size used to generate the genetic map, $(\varrho _{ij})$ is an estimate of the population-scaled recombination rate between SNPs i and j taken as $\varrho _{ij} = 4N_{\mathrm{e}}c_{ij}$, for Ne the effective population size and $c_{ij}$ the genetic distance between sites i and j in centimorgans as stated in Li and Stephens. LD matrix entries are set to zero if $exp(-\varrho _{ij}/2m)$ is less than a user-chosen cutoff.

Genetic distance between sites is derived from the genetic map files containing interpolated map positions for the CEU population generated from the 1000G OMNI arrays (Data availability). The calculation of the shrunk LD matrix requires the effective population sample size, which we set to be 11,400 (as in Zhu and Stephens), the sample size of the genetic map reference, which corresponds to the 183 individuals from the CEU cohort of the 1000G and the hard threshold on the shrinkage value, which we set to $10^{-3}$. This threshold gave a good balance between computational efficiency and accuracy with, on average, each SNP having a window width of 10.6 Mb (SD=5.6Mb) across the autosomes. The shrunk LD matrix is stored in a sparse matrix format (ignoring matrix elements equal to 0) for efficient SBayesR computation. Currently, the LD matrix construction can only be performed with PLINK hard-call genotypes.


### McMC algorithm for a single trait Bayesian model based on summary statistics  

The following steps are required, 1) reconstruct  \( X^{'}y \)  and  \( X'X \)  from summary statistics and LD reference panel as explained above, 2) initialise parameters, 3) sampling of model parameters from full conditional posterior distributions described above.

Initialize parameters:

\[ b=0 \]

\[  \sigma _{e}^{2}=\frac{sse}{n-1} \]

\[ sse=y^{'}y-2b'X'y+b^{'}X^{'}Xb=y^{'}y~~~~~~~~~~~~~ \left( given that b=0 initially \right)\] 

\[ y^{'}y=\frac{1}{m} \sum _{j=1}^{m} \left( y^{'}y \right) _{j} \]

\[  \left( y^{'}y \right) _{j}=\hat{ \sigma }_{b_{i}}^{2}x_{j}^{'}x_{j} \left( n-2 \right) +\hat{b}_{i}^{2}x_{j}^{'}x_{j} \]

\[ h_{snp}^{2}=0.5 \]

\[  \sigma _{g}^{2}= \sigma _{e}^{2}h_{snp}^{2} \]

\[  \sigma _{ \beta }^{2}=\frac{ \sigma _{g}^{2}}{ \left( 1- \pi _{c=1} \right)  \sum _{j=1}^{m}2p_{j} \left( 1-p_{j} \right) } \]


\[  \pi = \left( \text{0.995, 0.002, 0.002, 0.001} \right)  \]

\[  \gamma = \left( 0,0.01,0.1,1.0 \right)  \]

\[ \widetilde{v}_{ \beta }=\widetilde{v}_{e}=4 \] 

\[ \widetilde{S}_{ \beta }^{2}=\frac{ \left( \widetilde{v}_{ \beta }-2 \right)  \sigma _{ \beta }^{2}}{\widetilde{v}_{ \beta }}=\frac{ \sigma _{ \beta }^{2}}{2} \] 

\[ \widetilde{S}_{e}^{2}=\frac{ \left( \widetilde{v}_{e}-2 \right)  \sigma_{e}^{2}}{\widetilde{v}_{e}}=\frac{ \sigma _{e}^{2}}{2} \]

\( r^{\ast}=X^{'}y- \) \( X^{'}Xb \) 


\[  \left( r^{\ast}= X^{'}y~\text{because initially }b=0 \right)  \]


Gibbs sampler:


for i = 1 to number of iterations do

for j = 1 to number of markers do

 \( r_{j}=r_{j}^{\ast}+ x_{j}^{'}x_{j}b_{j}^{i}~~ \left( rhs for marker j  \right)  \)

 \(  \sigma _{c}^{2}= \sigma _{ \beta }^{2} \gamma _{d_{j}=c}~\text{for each of C classes } \left( e.g. 4 classes  \gamma = \left( 0,0.01,0.1,1.0 \right)   \right)  \)

 \( l_{jc}=x_{j}^{'}x_{j}+ \frac{ \sigma _{e}^{2}}{ \sigma _{c}^{2}}~~ \left( lhs for marker j in class c  \right)  \) 

 \( logL_{c}=0.5 \left[ \log  \left( \frac{ \sigma _{e}^{2}}{ \sigma _{c}^{2}l_{jc}} \right) +\frac{ \sigma _{c}^{2}}{ \sigma _{e}^{2}} \left( \frac{r_{j}^{2}}{ \sigma _{c}^{2}l_{jc}} \right)  \right] + log \left(  \pi _{c} \right)  \) \   \(  \left( \log likelihoodfor marker j in class c  \right)  \) 

 \( P \left( d_{j}=c \vert  \theta _{-j},\widetilde{y} \right) =\frac{1}{ \sum _{k=1}^{C}\exp  \left[ logL_{k}-logL_{c} \right] }~ \left( posterior probability for marker j in class c  \right)  \) \   \( \text{Sample variance class membership, d}_{j}\text{, for marker j using a categorical sampler } \) 

 \( \text{Sample marker effect,}b_{j}^{i+1}\text{, from N} \left( \frac{r_{j}}{l_{jc}},\frac{ \sigma _{e}^{2}}{l_{jc}} \right) \text{based on variance class membership }d_{j} \)  \par

 \[  \]  \[ \text{Update right hand side }r_{j}^{\ast \left( i+1 \right) }=r_{j}^{\ast \left( i \right) }- X_{j}^{'}x_{j} \left( b_{j}^{i+1}-b_{j}^{i} \right) ~where~X_{j}^{'}x_{j}\text{is the jth column of}~X^{'}X \]

enddo


Sample  \(  \sigma _{ \beta }^{2}~ \) from scaled inverse chi-squared distribution with  \(  \upsilon _{ \beta }=\widetilde{ \nu }_{ \beta }+q \)  degrees of freedom and scale parameter  \( S_{ \beta }^{2}=\frac{\widetilde{ \nu }_{ \beta }\widetilde{S}_{ \beta }^{2}+ \sum _{j=1}^{q}\frac{b_{j}^{2}}{ \gamma _{d_{j}}}}{\widetilde{ \nu }_{ \beta }+q} \)  where  \( \widetilde{ \nu }_{ \beta }~ \) and  \( \widetilde{S}_{ \beta }^{2} \)  are the prior degrees of freedom and scale parameters.


Sample $\sigma _{e}^{2}~$ from scaled inverse chi-squared distribution with $\upsilon_{e}=\widetilde{ \nu }_{e}+n$  degrees of freedom and scale parameter $S_{ \beta }^{2}=\frac{\widetilde{ \nu }_{e}\widetilde{S}_{e}^{2}+sse}{\widetilde{ \nu }_{e}+n}$ where $\widetilde{ \nu }_{e}~$ and $\widetilde{S}_{e}^{2}$ are the prior degrees of freedom and scale parameters and $sse=y^{'}y-b^{'}r^{\ast}-b^{'}X^{'}y$.

Sample  $\pi$ from a $Direchlet~\left( C,c + \alpha  \right)$, where $\bf{c}$ is a vector of length C that contains the counts of the number of variants in each variance class and $\alpha = \left( 1,1,..,1 \right)$.

Calculate  $\sigma _{g}^{2}=\frac{mss}{n} \)  where  \( mss=b^{'}X^{'}y-b^{'}r$ 

Calculate  $h_{snp}^{2}=\frac{ \sigma _{g}^{2}}{ \sigma _{g}^{2}+ \sigma _{e}^{2}}$  

enddo

# Bayesion Linear Regression Model for Individual Effects

## Statistical model and model parameters

We begin with a description of the general two-trait model since the simpler models are special cases. The general two-trait model partitions the total phenotypic variance into a component explained by genetic marker effects, $g$, and a component due to residual effects that are not associated with markers, $e$. Additionally, the component due to genetic markers $g$ is subdivided into contributions from $C$ marker sets $g_{i}$, $i=1,\ldots ,C$. 

The model for trait 1 is assumed to be

\begin{equation}
y_{1}=\mu_{1}+\sum\nolimits_{i=1}^{C}{g_1}_{i}+e_{1},  \label{em}
\end{equation}

and similar for trait 2,

\begin{equation}
y_{2}=\mu_{2}+\sum\nolimits_{i=1}^{C}{g_2}_{i}+e_{2},  \label{em}
\end{equation}

In these expressions, the $\mu$'s are scalar means for each trait, the $g^{\prime}$s are contributions to genetic effects from each of $C$ marker sets that can be associated with genetic marker information (genomic effects or genomic values), the $e^{\prime }s$ represent a
residual component that cannot be captured by regression on markers.

The genomic value for marker set $k$ is defined as the sum of the effects of all the markers in marker set $k$,

\begin{align}
g_1k &= \sum\nolimits_{i=1}^{p_{k}}{w}_{ijk}b_{1ik}  \notag \\
&= {w}_{jk}^{\prime }b_{1k},  \label{biv2}
\end{align}

with an equivalent expression for trait 2. In (\ref{biv2}), $p_{k}$ is the number of markers in marker set $k$, the scalar ${w}_{ijk}$ is the observed (centered and scaled) label for marker $i$ in individual $j$ for marker set $k$, and $b_{1ik}$ is the effect of marker $i$ for marker set $k$ of trait 1. 

The $n$ by $1$ vectors of genomic effects for trait 1 and trait 2 for marker set $k$ are $g_{1k}=W_{k}b_{1k}$ and $g_{2k}=W_{k}b_{2k}$, respectively, where $W_{k}=\left\{w_{ijk}\right\}$ is the observed $n$ by $p_{k}$ matrix of marker genotypes of marker set $k$. The joint distribution of vectors $b_{1k}$ and $b_{2k}$ is assumed to be


\begin{equation}
\left[
\begin{array}{c}
b_{1k} \\
b_{2k}
\end{array}
\right] \sim N\left( \left[
\begin{array}{c}
0 \\
0
\end{array}
\right] ,\left[
\begin{array}{cc}
I\sigma _{b_{1k}}^{2} & I\sigma _{b_{1k}b_{2k}} \\
I\sigma _{b_{1k}b_{2k}} & I\sigma _{b_{2k}}^{2}
\end{array}
\right] \right)  \label{biv1}
\end{equation}

where the $I^{\prime }$s represent $p_{k}$ by $p_{k}$ identity matrices, $\sigma_{b_{1k}}^{2}$ $\left(\sigma_{b_{2k}}^{2}\right)$ is the prior variance of marker effects for trait 1 (trait 2) in marker set $k$, and $\sigma_{b_{1k}b_{2k}}$ is their prior covariance.

Due to the centering the rank of $W_{k}$ is $n-1$. It follows from these assumptions and from standard properties of the multivariate normal distribution that the model for the joint distribution of genomic values in trait 1 and 2 is the singular multinormal ($SN$) distribution,

\begin{equation}
\left[
\begin{array}{c}
g_{1k} \\
g_{2k}
\end{array}
\right] \sim SN\left( \left[
\begin{array}{c}
0 \\
0
\end{array}
\right] ,\left[
\begin{array}{ll}
G_{k}\sigma_{g_{1k}}^{2} & G_{k}\sigma_{g_{1k}g_{fk}} \\
G_{k}\sigma_{g_{1k}g_{2k}} & G_{k}\sigma_{g_{2k}}^{2}
\end{array}
\right] \right) ,\quad k=1,\ldots ,C.  \label{biv3}
\end{equation}

Above, $\sigma_{g_{1k}}^{2}$ is the genomic variance in trait 1 for marker set $k$, $\sigma_{g_{2k}}^{2}$ is the genomic variance in trait 2 for marker set $k$, $\sigma_{g_{1k}g_{2k}}$ is the genomic covariance between trait 1 and 2 for marker set $k$, $G_{k}=\frac{1}{p_{k}} W_{k}W_{k}^{\prime}$ is the genomic relationship matrix of rank $n-1$ for marker set $k$, and $SN$ denotes the singular normal distribution (details in the Supplementary Methods). A more compact notation for (\ref{biv3}) is

\begin{equation}
g_{k}=\left( g_{1k}^{\prime },g_{2k}^{\prime }\right) ^{\prime }\sim
SN\left( 0,G_{k}\otimes Vg_{k}\right)  \label{biv31}
\end{equation}

where

\begin{equation}
{V_g}_k=\left[
\begin{array}{ll}
\sigma _{g_{1k}}^{2} & \sigma _{g_{1k}g_{2k}} \\
\sigma _{g_{1k}g_{2k}} & \sigma _{g_{2k}}^{2}
\end{array}
\right] .  \label{vgi}
\end{equation}

Similarly, collecting the $e^{\prime }s$ in vectors with $n$ elements for each trait joint distribution is assumed to be

\begin{equation}
\left[
\begin{array}{c}
e_{1} \\
e_{2}
\end{array}
\right] \sim N\left( \left[
\begin{array}{c}
0 \\
0
\end{array}
\right] ,\left[
\begin{array}{ll}
I\sigma_{e_{1}}^{2} & I\sigma_{e_{1}e_{2}} \\
I\sigma_{e_{1}e_{2}} & I\sigma_{e_{2}}^{2}
\end{array}
\right] \right) ,  \label{eh}
\end{equation}

The identity matrix $I$ is of order $n$ by $n$ in (\ref{eh})\ . The phenotypes for trait 1 and 2, $y_{1}$ and $y_{2}$, each with $n$ elements, are conditionally normally distributed, given $g$, and $e$, and take the form

\label{y}
\begin{eqnarray}
y_{1}|\mu_{1},g_{1} &\sim & N\left(1\mu_{1}+\sum\nolimits_{i=1}^{C}g_{1i},I\sigma_{e_1}^{2}\right) ,
\label{ym} \\
y_{2}|\mu_{2},g_{2} &\sim & 
N\left( 1\mu_{2}+\sum\nolimits_{i=1}^{C}g_{2i},I\sigma_{e_2}^{2}\right) ,
\label{yf}
\end{eqnarray}

where $\sigma_{e1}^{2}$ ($\sigma_{e2}^{2}$) is the residual variance for the trait 1 (trait 2). 


## Extensions to multiple components
The model specified quantify the relative contribution from each of the marker sets by fitting separate variance components for each of those marker sets. Here we describe the basis for this partitioning. The point of departure is to assume that the $2n_{t}$ by $1$ vector of genomic effects $g=\left(g_{1}^{\prime},g_{2}^{\prime}\right)^{\prime}$ is equal to the sum of the vectors of genomic effects belonging to $C$ marker set components. Then 

\begin{eqnarray}
g =\sum_{i=1}^{C}g_{i},Var\left( g|W\right) =\sum_{i=1}^{C}Var\left( g_{i}|W_{i}\right)
=\sum_{i=1}^{C}\frac{1}{p_{i}}W_{i}W_{i}^{\prime}\otimes {V_g}_i
\end{eqnarray}

where $g_{i}$ is the $2n$ by $1$ vector of genomic values for trait 1 and 2, associated with component $i$, $i=1,\ldots,C$, $p_{i}$ is the number of marker genotypes in component $i$, and ${V_g}_i$ is the $2$ by $2$ genomic covariance matrix for marker set $i$. This holds under the model assumption that the components of the vector of marker effects $b=\left( b_{1}^{\prime },\ldots ,b_{i}^{\prime },\ldots b_{C}^{\prime }\right) ^{\prime }$ are realisations from $b_{i}\sim N\left( 0,I\otimes {V_b}_i\right)$, with $Cov\left(b_{i},b_{j}^{\prime }\right) =0$, $i,j=1,\ldots ,C;i\neq j$. Then $Cov\left(g_{i},g_{j}^{\prime }|W\right)=Cov\left( W_{i}b_{i},b_{j}^{\prime}W_{j}^{\prime }\right)=0$. In these expressions, $b_{i}=\left({b_1}_{i}^{\prime },{b_2}_{i}^{\prime }\right) ^{\prime}$ is the vector of marker effects of marker set component $i$, and

\begin{equation}
{V_b}_i=\left[\begin{array}{cc}
{\sigma_{b_1}}_{i}^{2} & \sigma_{{b_1}_{i} {b_2}_{i}} \\
\sigma_{{b_1}_{i}{b_2}_{i}} & {\sigma_{b_2}}_{i}^{2}
\end{array}\right].
\end{equation}

Two simple two-trait models are implemented that partition the total genomic variance into multiple component defined by marker sets. Details of the prior distributions of the parameters of the Bayesian models and of the Markov chain Monte Carlo algorithm can be found in the Supplementary Methods.


### Inferences
For single trait analysis, inferences are reported in terms of ratios for the ith marker sets defined as:

\begin{equation}
h_{g_i}^{2}=\frac{\sigma_{{g}_{i}}^{2}}{\sigma_{{g}}^{2}},\quad i=1,\ldots ,C.
\end{equation}

with $\sigma_{{g}}^{2}=Var\left(\sum\nolimits_{i=1}^{C}g_{i}\right)$. When the elements of $W$ are scaled so that the average of the diagonal elements of $WW^{\prime}$ is equal to $1$, this ratio quantifies the proportion of total genomic variance, captured by genetic marker information associated with marker set $i$. 
The ratio involving marker effects from all the marker sets for trait 1 is

\begin{equation}
h_{g_1}^{2}=\frac{\sigma_{g_1}^{2}}{\sigma_{g_1}^{2}+\sigma_{e_1}^{2}}  
\end{equation}

with a similar expression for trait 2.

We also report within marker set genomic correlations

\begin{equation}
r_{g_{i}}=\frac{\sigma_{g_{i_1}g_{i_2}}}{\sigma_{{g}_{i_1}}\sigma _{{g}_{i_2}}}
,\quad i=1,\ldots ,C,  
\end{equation}

and the total genomic correlation between traits, defined as

\begin{equation}
r_{g_{i}}=\frac{Cov\left(
\sum\nolimits_{i=1}^{C}g_{1_{i}},\sum\nolimits_{i=1}^{C}g_{2_{i}}\right)}{\sqrt{
\sigma _{{g}_{1}}^{2}\sigma _{{g}_{2}}^{2}}}
\end{equation}

The absence of symmetry of posterior distributions is well captured by the Bayesian McMC methods. In a situation like this, summarizing results in the form of posterior means and standard deviations would be misleading. In the presence of asymmetry, posterior means are poor indicators of points of high probability mass. We therefore summarize inferences in terms of posterior modes and posterior intervals.

## Estimation of model parameters

The models are implemented using empirical Bayesian methods. The models were implemented using a Bayesian, Markov chain Monte Carlo approach, whereby the hyperparameters of the dispersion parameters of the Bayesian model were estimated by maximum likelihood, and conditional on these, the model was fitted using Markov chain Monte Carlo. To illustrate, consider determining a value of the scale parameter of an inverse chi square distribution, which is the prior assumed for the variance components. This is achieved by equating the mode of the inverse chi square prior density, for a fixed value of the degrees of freedom, to the maximum likelihood estimate and solving for the scale parameter. The decision to huse this approach rather than classical likelihood is based on the need to obtain measures of uncertainty that account as much as possible for the limited amount of information in the data without resorting to large sample theory, which is inherent in classical likelihood. This is relevant if the data are of limited size and results in marginal posterior distributions that are expected to be asymmetric, particularly in the case of the more complex, general two-trait model. 

A number of technical details are presented in this section. These include a formal derivation of the probability model for the variance between traits and details of a spectral decomposition that plays an important computational role in the Markov chain Monte Carlo strategy implemented, as well as a full description of the Bayesian models. 

### The spectral decomposition 
The Bayesian implementation of the model is facilitated making use of the following spectral decomposition performed within each chromosome segment (we drop subscripts that refer to marker sets to avoid clotting the notation). Let


\begin{equation}
g_{1}=Wb_{1} \\
g_{2}=Wb_{2}
\end{equation}

where $W$ is of order $n\times p$ and $g_{m}$ is of order $n\times 1$. The spectral decomposition of $WW^{\prime}$ (for each marker set) is

\begin{eqnarray}
WW^{\prime } &= U\Delta U^{\prime} &= \sum\nolimits_{i=1}^{n_{t}}\lambda_{i}U_{i}U_{i}^{^{\prime }},
\end{eqnarray}

where $U=\left[U_{1},U_{2},\ldots ,U_{n_{t}}\right]$, of order $n\times n$ is the matrix of eigenvectors of $WW^{\prime}$, $U_{j}$ is the jth column (dimension $n\times 1$), and $\Delta$ is a diagonal matrix with elements equal to the eigenvalues $\lambda_{1},\lambda_{2},\ldots,\lambda_{n_{t}}$ associated to the $n_{t}$ eigenvectors. Since $WW^{\prime}$ is positive semidefinite the eigenvalues are $\lambda_{i}\geq 0$, $i=1,2,\ldots ,n$. The eigen vectors satisfy $U^{\prime}U=UU^{\prime}=I$.


Define the $n\times n$ matrix $G=\frac{1}{p}WW^{\prime}=$ $\frac{1}{p}WW^{\prime }$ and write this as
\begin{eqnarray*}
G &= \frac{1}{p}WW^{\prime } \\
&= \frac{1}{p}U\Delta U^{\prime } \\
&= UDU^{\prime }
\end{eqnarray*}
where
\begin{equation*}
D=\frac{1}{p}\Delta .
\end{equation*}

Matrix $G$ (peculiar to each marker set) is singular and the diagonal matrix $D$ (also peculiar to each marker set) contains only $n-1$ positive eigenvalues.

\subsubsection{The case of singular $G$ }

Due to the singularity of $G$, $\left[ g|W,\sigma_{g}^{2}\right]$ does not follow a multivariate normal distribution but a singular normal distribution instead. The singular normal density is

\begin{equation}
p\left( g_{1}|W,\sigma_{g1}^{2}\right) =\frac{1}{\left(
2\pi \right) ^{\frac{n-1}{2}}\left( \lambda_{1}\sigma_{g_{1}}^{2}\ldots \lambda_{n-1}\sigma_{g_{1}}^{2}\right) ^{\frac{1}{2}}}\exp \left( -\frac{g_{1}^{\prime }G^{-}g_{1}}{
2\sigma_{g_{1}}^{2}}\right)  \label{e7}
\end{equation}

where the $n-1$ $\lambda^{\prime}s$ are the non-zero eigenvalues of $G$ and $G^{-}$ is any generalised inverse of $G$. One choice choice of generalised inverse of $G$ is

\begin{equation}
G^{-}=UD^{-}U^{\prime }
\end{equation}

where $U$ and $D$ are of order $n_{t}$ by $n_{t}$

\begin{equation*}
D^{-}=\frac{1}{p}\left[
\begin{array}{ccccc}
\frac{1}{\lambda_{1}} & 0 & \ldots & \ldots & 0 \\
0 & \ddots & \ldots & \ldots & 0 \\
\vdots & \vdots & \frac{1}{\lambda_{n-1}} & \ldots & 0 \\
\vdots & \vdots & \ldots & \ddots & 0 \\
0 & 0 & 0 & 0 & 0
\end{array}
\right] =\frac{1}{p}\left[
\begin{array}{cc}
D_{1}^{-1} & 0 \\
0^{\prime } & 0
\end{array}
\right]
\end{equation*}

Above, $D_{1}=diag\left( \lambda_{i}\right)_{i=1}^{n-1}$, a diagonal matrix of dimension $n-1$ by $n-1$ that contains the nonzero eigenvalues $\lambda _{i}$. The remaining elements of $D^{-}$ are all equal to zero.

\subsubsection{A probabilistically equivalent reparameterisation of the random regression model}

We describe a reparameterisation of the original two-trait model that simplifies the Markov chain Monte Carlo computations.

For each marker set (omitting the subscripts) define the row vector random variable $\alpha^{\prime}=\left(\alpha_{1}^{\prime},\alpha_{2}^{\prime}\right)$ of dimension $1$ by $2n$, with vectors $\alpha_{1}$ and $\alpha_{2}$, each of dimension $n$ by $1$ associated with trait 1 and 2, with distribution

\begin{equation}
\left(
\begin{array}{c}
\alpha _{1} \\
\alpha _{2}
\end{array}
\right) \sim SN\left( \left[
\begin{array}{c}
0 \\
0
\end{array}
\right] ,\left[
\begin{array}{cc}
D\sigma_{g_{1}}^{2} & D\sigma_{g_{1}g_{2}} \\
D\sigma_{g_{1}g_{2}} & D\sigma_{g_{2}}^{2}
\end{array}
\right] \right)  \label{alf1}
\end{equation}

where $D$ is a diagonal matrix (associated with a particular marker set) of dimension $n\times n$, with has eigenvalues $\lambda_{i}$, $i=1,\ldots ,n$ as diagonal elements, of which the first $n-1$ are positive and the rest are equal to zero. We define for each marker set
\begin{equation}
V_{g}=\left[
\begin{array}{cc}
\sigma_{g_{1}}^{2} & \sigma_{g_{1}g_{2}} \\
\sigma_{g_{1}g_{2}} & \sigma_{g_{1}}^{2}
\end{array}
\right] .  \label{alf2}
\end{equation}

In a particular marker set, the random variables
\begin{equation}
\left(
\begin{array}{c}
U\alpha_{1} \\
U\alpha_{2}
\end{array}
\right) \sim SN\left( \left[
\begin{array}{c}
0 \\
0
\end{array}
\right] ,\left[
\begin{array}{cc}
UDU^{\prime}\sigma_{g_{1}}^{2} & UDU^{\prime}\sigma_{g_{1}g_{2}} \\
UDU^{\prime}\sigma_{g_{1}g_{2}} & UDU^{\prime}\sigma_{g_{2}}^{2}
\end{array}
\right] \right)  \label{ualfa}
\end{equation}
and
\begin{equation}
\left(
\begin{array}{c}
g_{1} \\
g_{2}
\end{array}
\right) \sim SN\left( \left[
\begin{array}{c}
0 \\
0
\end{array}
\right] ,\left[
\begin{array}{cc}
\frac{1}{p}WW^{\prime}\sigma_{g_{1}}^{2} & \frac{1}{p}
WW^{\prime }\sigma_{g_{1}g_{2}} \\
\frac{1}{p}WW^{\prime}\sigma_{g_{1}g_{2}} & \frac{1}{p}
WW^{\prime }\sigma_{g_{2}}^{2}
\end{array}
\right] \right)  \label{alfazg}
\end{equation}

have the same distribution since $UDU^{\prime }=\frac{1}{p}WW^{\prime }$. Here $p$ is the number of genetic markers in the marker set.
Therefore the structure can be written as

\label{yt}
\begin{eqnarray}
y_{1}|\mu_{1},\sum\nolimits_{i=1}^{C}\alpha_{1i} &\sim
&N \left( 1\mu_{1}+\sum\nolimits_{i=1}^{C}U_{i}\alpha_{1i}, I\sigma_{1e}^{2}\right),  \label{ytm} \\
y_{2}|\mu_{2},\sum\nolimits_{i=1}^{C}\alpha_{2i} &\sim
&N \left( 1\mu_{2}+\sum\nolimits_{i=1}^{C}U_{i}\alpha_{2i}, I\sigma_{2e}^{2}\right) \quad i=1,\ldots ,C.  \label{ytf}
\end{eqnarray}


\subsubsection{Prior distributions}

The prior distribution of the $\mu ^{\prime }s$ is $N\left( 0,10^{5}\right)$. The prior distributions of $V_{g}$ and $V_{h}$ are scaled inverted Wishart with degrees of freedom set equal to $2.5$ and scale parameters $P_{g}$ and $P_{h}$, respectively. The degrees of freedom generate a proper distribution with overdispersed values. The prior distributions of $\sigma_{r_{m}}^{2}$ and $\sigma_{r_{f}}^{2}$ are scale inverted chi square densities. The degrees of freedom of these densities are set equal to $1.0$ which leads to vague prior information. For example, when the scale is equal to $0.1$, the modal value of the prior distribution is $0.03$ and the prior probability that the variance component is smaller than this value is $56$. The prior probability that the variance component is between $0.03$ and $0.3$ is $29$.

The scale parameters of all these distributions are estimated using maximum likelihood. This involved obtaining maximum likelihood estimates of the two-trait model (for Models $H$ and $G$, each sex considered as one trait), and then equating these estimates to the mode of the relevant prior distribution, written as a function of the scale parameter. In the case of the general two-trait model, single-trait likelihoods were fitted instead. The off-diagonal elements of the scale parameter of inverse Wishart distributions were set equal to zero.

\subsubsection{Fully conditional posterior distributions}

The fully conditional posterior distributions of a parameter $\theta$ is denoted $\left[\theta|All,z\right]$ where $All$ denotes all the parameters of the model except $\theta$. Here we sketch the form of these densities.


\paragraph{Updating the $\alpha^{\prime}s$}

with subscripts $1$ for trait 1, $2$ for trait 2 and $k$ for marker set, from the bivariate normal distribution 

\begin{equation*}
\left[ \left.
\begin{array}{c}
\alpha _{ki1} \\
\alpha _{ki2}
\end{array}
\right\vert All,y\right] \sim N\left( \left[
\begin{array}{c}
\widehat{\alpha }_{ki1} \\
\widehat{\alpha }_{ki2}
\end{array}
\right] ,\left[ I+\frac{\sigma_{e}^{2}}{\lambda _{ki}}\left(
\begin{array}{cc}
\sigma _{g_{1}}^{2} & \sigma _{g_{1}g_{2}} \\
\sigma _{g_{1}g_{2}} & \sigma _{g_{2}}^{2}
\end{array}
\right) _{k}^{-1}\right] ^{-1}\sigma _{e}^{2}\right) ,\quad i=1,2,\ldots
,n-1,
\end{equation*}
where
\begin{equation*}
\left( I+\lambda _{ki}^{-1}V_{g_{k}}^{-1}\sigma _{e}^{2}\right) \widehat{
\alpha }_{is}=\left[
\begin{array}{c}
U_{ki}^{\prime }\left( y_{1}-1\mu_{1}-\sum\nolimits_{i\neq k}U_{i}\alpha
h_{i1}\right)  \\
U_{ki}^{\prime }\left( y_{2}-1\mu_{2}-\sum\nolimits_{i\neq k}U_{i}\alpha
_{i2}\right)
\end{array}
\right] ,\qquad \alpha _{is}^{\prime }=\left( \alpha _{i1},\alpha
_{i2}\right).
\end{equation*}

The strategy updates jointly the $\alpha^{\prime}s$ for both traits from a given marker set, conditional on the $\alpha^{\prime}s$ from the remaining marker sets.

\paragraph{Updating $\mu_{1}$ and $\mu_{2}$}

The prior distribution of both scalars $\mu_{1}$ and $\mu_{2}$ are the normal process $N\left(0,10^{5}\right)$. The fully conditional for $\mu_{1}$ is proportional to 

\begin{equation*}
\left[\mu_{1}|All,x\right] \sim N\left(\widehat{\mu}_{1},\left(1^{\prime}1+k_{\mu_{1}}\right) ^{-1}\sigma_{e}^{2}\right)
\end{equation*}

where

\begin{equation*}
\left( 1^{\prime }1+k_{\mu_{1}}\right) \widehat{\mu}_{1}=1^{\prime }\left(y_{m}-U\alpha _{m}-T\gamma_{1}\right), \qquad k_{\mu_{1}}=\frac{\sigma_{e}^{2}}{10^{5}}.
\end{equation*}

A similar expression holds for $\mu_{2}$.


\paragraph{Updating $V_{g}$}

For each of the marker sets, the update involves drawing samples from scaled inverse Wishart distributions. The fully conditional of the $2\times 2$ matrix $V_{g}$ is proportional to

\begin{equation}
\left[\alpha_{m},\alpha_{f}|V_{g},D\right] \left[ V_{g}|\nu _{g},P_{g}\right].  
\end{equation}

The second term is inverse Wishart the prior distribution of $V_{g}$, $IW\left(\nu_{g},P_{g}\right)$ with density 
\begin{equation} p\left( V_{g}|\nu _{g},P_{g}\right) \propto \left\vert Vg\right\vert ^{-
\frac{1}{2}\left( v_{g}+3\right) }\exp \left[ -\frac{1}{2}tr\left(
V_{g}^{-1}P_{g}\right) \right]  \label{priorvg}
\end{equation}

where the hyperparameters $\nu_{g}$ and $P_{g}$ are the degrees of freedom and the scale, respectively. The modal value of this distribution is given by $P_{g}/(\nu_{g}+p+1)$, where in our case, $p=2.$ On defining (see \citealp{sorensenandgianola2002}, page 574)

\begin{equation*}
S_{g}=\left[
\begin{array}{cc}
\alpha_{1}^{\prime }D^{-1}\alpha_{1} & \alpha_{1}^{\prime }D^{-1}\alpha_{2} \\
\alpha_{2}^{\prime }D^{-1}\alpha_{1} & \alpha_{2}^{\prime }D^{-1}\alpha_{2}
\end{array}\right],
\end{equation*}

the density of the fully conditional posterior distribution of $V_{g}$ is


\begin{eqnarray}
p\left( V_{g}|All,y\right) &\propto & \left\vert V_{g}\right\vert ^{-\frac{k}{
2}}\left\vert Vg\right\vert ^{-\frac{1}{2}\left( v_{g}+3\right) }\exp \left[
-\frac{1}{2}tr\left( V_{g}^{-1}P_{g}\right) \right] \exp \left[ -\frac{1}{2}
tr\left( V_{g}^{-1}S_{g}\right) \right]  \notag \\
&= & \left\vert Vg\right\vert ^{-\frac{1}{2}\left( k+v_{g}+3\right) }\exp 
\left[ -\frac{1}{2}tr\left[ V_{g}^{-1}\left( S_{g}+P_{g}\right) \right]
\right]  \label{iw1}
\end{eqnarray}

where $k=n-1$, which is in the form of an inverse Wishart distribution of dimension $2$, $k+v_{g}$ degrees of freedom and scale matrix $\left( S_{g}+P_{g}\right)$.

\paragraph{Updating $V_{e}$}

The update here is similar and it involves again drawing samples from scaled inverse Wishart distributions. The fully conditional posterior distribution of the $2\times 2$ matrix $V_{e}$ defined in (\ref{vh}) is proportional to

\begin{equation}
\left[ e_{1},e_{2}|V_{e},E\right] \left[ V_{e}|\nu_{e},P_{e}
\right]  \label{fcvh1}
\end{equation}

where the second term is the prior distribution of $V_{e}$ with hyperparameters $\nu_{e}$ and $P_{e}$ of the form

\begin{equation}
p\left( V_{e}|\nu_{e},P_{e}\right) \propto \left\vert V_{e}\right\vert ^{-
\frac{1}{2}\left( v_{e}+3\right) }\exp \left[ -\frac{1}{2}tr\left(
V_{e}^{-1}P_{e}\right) \right]  \label{priorve}
\end{equation}
symbolised $IW\left( \nu_{e},P_{e}\right)$. 


The density is

\begin{equation*}
p\left( V_{e}|All,y\right) \propto \left\vert V_{e}\right\vert ^{-\frac{1}{2}
\left( n+v_{e}+3\right) }\exp \left[ -\frac{1}{2}tr\left[
V_{e}^{-1}\left( S_{e}+P_{e}\right) \right] \right],
\end{equation*}

an inverse Wishart distribution of dimension $2$, $n+v_{e}$ degrees of freedom and scale matrix $\left(S_{e}+P_{e}\right)$, where

\begin{equation*}
S_{h}=\left[
\begin{array}{cc}
\gamma_{m}^{\prime }E^{-1}\gamma_{m} & \gamma_{m}^{\prime }E^{-1}\gamma
_{f} \\
\gamma_{f}^{\prime }E^{-1}\gamma_{m} & \gamma_{f}^{\prime }E^{-1}\gamma
_{f}
\end{array}
\right].
\end{equation*}

