---
title: "Bayesian Linear Regression Models"
author: "Palle Duun Rohde, Izel Fourie Sørensen & Peter Sørensen"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    citation_package: natbib
    includes:
      in_header: preamble.tex
  html_document:
    number_sections: yes
    includes:
      in_header: mathjax_header.html
  word_document: default
bibliography: [qg2021.bib, book.bib, packages.bib]
link-citations: yes
---

# Introduction

Bayesian linear regression models have been proposed as a unified framework for gene mapping, prediction of genetic predispostion, estimation of genetic parameters and effect size distribution ([Moser et al. 2015](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4388571/)). 

Bayesian linear regression models attempts to account for the underlying genetic architecture of the trait. This is achieved by using many linked markers covering the entire genome to jointly estimate marker effects, and by allowing the genetic signal to be heterogeneous distributed over the genome (i.e. some regions have stronger genetic signal than others). This may in some situations allow a more accurate estimate of the true underlying genetic signal leading to more accurate predictions. 

Bayesian linear regression models can also be used to map genetic variants associated with phenotypes and to estimate the total variance explained by the genetic markers. Because they fit all markers simultaneously and account for linkage disequilibrium between markers, they should have greater power to detect true associations, find less false negatives and give unbiased estimates of the larger marker effects. They can also provide information about the genetic architecture of the trait from the hyper-parameters of the distribution of marker effects.

Bayesian linear regression models fit all markers simultaneously and their effects as drawn from a prior distribution that attempts to match the true distribution of marker effects as closely as possible. However the true distribution of effect sizes is unknown but a mixture of normal distributions can approximate a wide variety of distributions by varying the mixing proportions. Erbe et al. used this prior and included one component of the mixture with zero variance. A similar model was proposed by Zhou et al. but with a mixture of two normal distributions, one with a small variance and one with a larger variance.

In the following the statistical model, prior distributions of model parameters, algorithms for estimation of model parameters, extensions for handling multiple marker sets and multiple traits for the Bayesian linear regression models is presented. We will present two alternative BLR models, BLR model for marker effects and BLR model for individual effects.  


# Bayesion Linear Regression Model for Marker Effects

## Statistical model and model parameters
In the multiple regression model the phenotype is related to the set of genetic markers:

\begin{align}
y = Xb + e
\end{align}

where $y$ is the phenotype, $X$ a matrix of SNP genotypes, where values are standardised to give the ijth element as: $x_{ij} = \left( {x_{ij} - 2p_j} \right){\mathrm{/}}\sqrt {2p_j\left( {1 - p_j} \right)}$, with $x_{ij}$ the number of copies of the effect allele (e.g. 0, 1 or 2) for the ith individual at the jth SNP and $p_j$ the allele frequency of the effect allele. $b$ are the genetic effects for each SNP, and $e$ the residual error. The dimensions of $y$, $X$, $b$ and $e$ are dependent upon the number of traits, $k$, the number of SNP markers, $m$, and the number of individuals, $n$. The residuals, $e$, are a priori assumed to be independently and identically distributed multivariate normal with null mean and covariance matrix $I\sigma_{e}^2$. 


## Estimation of parameters using Bayesian methods
In the Bayesian multiple regression model the posterior density of the model parameters ($b$,$\sigma_b^2$,$\sigma_e^2$) depend on the likelihood of the data given the parameters and a prior probability for the model parameters:
\begin{align}
p(b,\sigma_b^2,\sigma_e^2|y) \propto p(y|b,\sigma_b^2,\sigma_e^2)p(b|\sigma_b^2)p(\sigma_b^2)p(\sigma_e^2)
\end{align}

The prior density of marker effects, $p(b|\sigma_b^2)$, defines whether the model will induce variable selection and shrinkage or shrinkage only. Also, the choice of prior will define the extent and type of shrinkage induced. Ideally the choice of prior for the marker effect should reflect the genetic architecture of the trait, and will vary (perhaps a lot) across traits. Most complex traits and diseases are likely highly polygenic, with hundreds to thousands of causal variants, most frequently of small effect. So, the prior distribution must include many small and few large effects. Furthermore marker effects are a priory assumed to be uncorrelated (but markers can be in strong linkage disequilibrium and therefore a high posterior correlation). Many priors for marker effects have been proposed. These priors come more from practical (ease of computation) than from biological reasons. Each prior originates a method or family of methods, and we will describe some of them next, as well as their implications.


### Prior marker variance Bayes N {.unlisted .unnumbered}
In the Bayes N approach the prior the marker effect, b, follows a priori a normal distribution with a variance $\sigma_b^2$ which is constant across markers:
\begin{align}
p(b)=\prod_ip(b_i)
\end{align}
where
\begin{align}
p(b_i) = N\left(0, \sigma_b^2 \right)
\end{align}
In a normal distribution most effects are concentrated around 0, whereas few effects will be large. Therefore, the prior assumption of normality precludes few markers of having very large effects – unless there is a lot of information to compensate for this prior information. 


### Prior marker variance Bayes A {.unlisted .unnumbered}
In the Bayes A approach it is assumed that a priori we have some information on the marker variance. For instance, this can be $\sigma_b^2$. Thus, we may attach some importance to this value and use it as prior information for $\sigma_{{b}_i}^2$. A natural way of doing this is using an inverted chi-squared distribution with with  $\upsilon_{b}$ degrees of freedom and scale parameter  $S_{b}^{2}=\upsilon_{b}\sigma_b^2$ 
\begin{align}
p(b_i|\sigma_{{b}_i}^2) = N\left(0, \sigma_{{b}_i}^2 \right)
\end{align}
In the second stage, we postulate a prior distribution for the variance themselves:
\begin{align}
p(\sigma_{{b}_i}^2 |\upsilon_{b},S_{b}^{2}) = S_{b}^{2}\chi_{\upsilon_{b}}^{-1}
\end{align}
The value of $\sigma_b^2$ should be set as $\sigma_b^2=\frac{\upsilon_{b}-2}{\upsilon_{b}}\frac{\sigma_g^2}{2\sum_{i}p_i(1-p_i)}$ because the variance of a t distribution is $\frac{\upsilon_{b}}{\upsilon_{b}-2}$. It can be shown that this corresponds to a prior on the marker effects corresponding to a scaled t distribution (Gianola et al. 2009):
\begin{align}
p(b_i|\sigma_{b}^2,\upsilon_{b}) = \sigma_{b}t\left(0, \upsilon_{b} \right)
\end{align}
which has the property of having “fat tails”. This means that large marker effects are more likely a priori compared to a normal distribution.


### Prior marker variance Bayes C {.unlisted .unnumbered}
In the Bayes C approach the marker effects, b, are a priori assumed to be sampled from a mixture with a point mass at zero and univariate normal distribution conditional on common marker effect variance  $\sigma_{b}^2$. This reflect a very common thought was that there were not many causal loci. This can be implemented by introducing additional variables $\delta_i$ which explain if the i-th marker has an effect or not. In turn, these variables $\delta$ have a prior distribution called Bernouilli with a probability $\pi$ of being 0. Therefore the hierarchy of priors is:
\begin{align}
p(b_j|\delta_i,\sigma_{{b}_i}^2,\pi) = \left\{ {\begin{array}{*{20}{l}} 0 \hfill & {{\mathrm{with}}\,{\mathrm{probability}}\,\pi,} \hfill \\ {\sim N(0,\sigma_{{b}_i}^2)} \hfill & {{\mathrm{with}}\,{\mathrm{probability}}\,1-\pi,}  \end{array}} \right.
\end{align}

\begin{align}
p(\sigma_{{b}_i}^2 |\upsilon_{b},S_{b}^{2}) = S_{b}^{2}\chi_{\upsilon_{b}}^{-1}
\end{align}
where $S_{b}^{2}=\sigma_b^2\upsilon_{b}$ with $\sigma_b^2=\frac{\sigma_g^2}{(1-\pi)2\sum_{i}p_i(1-p_i)}$ because the variance of a t distribution is $\frac{\upsilon_{b}}{\upsilon_{b}-2}$. 


### Prior marker variance Bayes R {.unlisted .unnumbered}
In the Bayes R approach the marker effects, b, are a priori assumed to be sampled from a mixture with a point mass at zero and univariate normal distributions conditional on common marker effect variance  $\sigma_{b}^2$,and variance scaling factors, ${\gamma}$:
\begin{align}
b_j|\pi ,\sigma _b ^2 = \left\{ {\begin{array}{*{20}{l}} 0 \hfill & {{\mathrm{with}}\,{\mathrm{probability}}\,\pi _1,} \hfill \\ {\sim N(0,\gamma _2\sigma _b ^2)} \hfill & {{\mathrm{with}}\,{\mathrm{probability}}\,\pi _2,} \hfill \\ \vdots \hfill & {} \hfill \\ {\sim N(0,\gamma _C\sigma _b ^2)} \hfill & {{\mathrm{with}}\,{\mathrm{probability}}\,1 - \mathop {\sum}\nolimits_{c = 1}^{C - 1} {\pi _c,} } \hfill \end{array}} \right.
\end{align}
where  $\pi = \left(\pi _{1}, \pi _{2}, ...., \pi _{C} \right)$  is a vector prior probabilities and  $\gamma = \left(  \gamma _{1}, \gamma _{2}, ....., \gamma _{C} \right)$ is a vector of variance scaling factors for each of C marker variance classes. The $~\gamma$ coefficients are prespecified and constrain how the common marker effect variance $\sigma _{b }^{2}$ scales within each mixture distribution. Typically $\gamma = \left( 0,0.01,0.1,1.0 \right)$. and  $\pi=\left(0.95,0.02,0.02,0.01\right)$. 

The prior distribution for the marker variance $\sigma_{b}^{2}$  is assumed to be an inverse Chi-square prior distribution,$\chi^{-1}\left(S_{b},\nu_{b}\right)$.

The proportion of markers in each mixture class $\pi$ follows a Direchlet $(C,c+\alpha)$ distribution, where c is a vector of length C that contains the counts of the number of variants in each variance class and $\alpha=(1,1,1,1)'$.   

Using the concept of data augmentation, an indicator variable ${d}=(d_1,d_2,..,d_{m-1},d_m)$, is introduced, where $d_j$ indicates whether the j’th marker effect is zero or nonzero.

### Estimation of model parameters
Bayesian linear regression methods use an iterative algorithm for estimating joint marker effects. Estimation of the joint marker effects depend on additional model parameters such as a probability of being causal ($\pi$), an overall marker variance ($\sigma_{b}^2$), and residual variance ($\sigma_e^2$). Estimation of model parameters can be done using MCMC techniques by sampling from fully conditional posterior distributions.

To illustrate consider the following MCMC algorithm used to obtain estimates of parameters in BayesR models.
The multiple linear regression model is parameterized in terms of  $\theta= (  \sigma_{e}^{2}, \sigma_{ \beta}^{2}, \gamma , \pi ,~b,~d)$. The full conditional sampling distributions for these parameters are presented below. 

The joint posterior for all parameter in the multiple regression model can be written as :

\begin{align}
 f \left(  \sigma _{e}^{2}, \sigma _{ \beta }^{2}, \gamma , \pi ,~b,~d \vert y \right) \propto \left( y \vert  \sigma _{e}^{2}, \sigma _{ \beta }^{2}, \gamma , \pi ,~b,~d \right) f \left( b \vert d, \gamma , \sigma _{ \beta }^{2} \right) f \left( d \vert  \pi  \right) f \left(  \sigma _{e}^{2} \right) f \left(  \sigma _{ \beta }^{2} \right) f \left(  \pi  \right)
\end{align}

The parameters  $d_{j}$  and  $b_{j}$  are sampled jointly from their joint full conditional distributions, which can be written as the product of the full conditional distribution of  $b_{j}$  given  $d_{j}$  and the marginal full conditional distribution of  $d_{j}$: 

\begin{align}
f_{j} \left( d_{j},b_{j} \vert  \theta _{-j},\widetilde{y} \right) =f_{j} \left( b_{j} \vert  \theta _{-j},d_{j},\widetilde{y} \right) f_{j} \left( d_{j}, \vert  \theta _{-j},\widetilde{y} \right)
\end{align}

Where $\theta _{-j}$ is all parameters except $d_{j}$  and $b_{j}$, and $\widetilde{y}=y-X_{-j}b_{-j}$ is the phenotype adjusted for all marker effects except for j'th marker ($b_{j}$). 


The full conditional distribution for \( b_{j}\) can be be written as:
\begin{align}
f_{j} \left( b_{j} \vert  \theta _{-j},d_{j},\widetilde{y} \right) \propto N \left( C_{j}^{-1}r_{j},C_{j}^{-1} \right) 
\end{align}

where   $C_{j}^{-1}=\frac{\sigma_{e}^{2}}{x_{j}^{'}x_{j}+\frac{\sigma_{e}^{2}}{\sigma_{c}^{2}}}$ and $r_{j}=\frac{x_{j}^{'}\widetilde{y}}{\sigma _{e}^{2}}$ 


The marginal full conditional probability for the indicator variable of  $d_{j}$ is:
\begin{align}
f_{j} \left( d_{j}=c \vert  \theta _{-j},\widetilde{y} \right) =\frac{f_{j} \left( \widetilde{y} \vert d_{j}=c, \theta _{-j} \right) f \left( d_{j} \vert  \pi _{c} \right) }{ \sum _{k=1}^{C}f_{j} \left( \widetilde{y} \vert d_{j}=k, \theta _{-j} \right) f \left( d_{j} \vert  \pi _{k} \right) } 
\end{align}

where $f_{j} \left( \widetilde{y} \vert d_{j}=c, \theta _{-j} \right) = \left(  \sigma _{c}^{2} \right) ^{-0.5} \left( C_{j}^{-1} \right) ^{0.5}\exp  \left[ \frac{1}{2}C_{j}^{-1}r_{j}^{2} \right]$ 

From these expression probabilities for the categorical distribution for an arbitrary number of mixture components can be computed. Given these probabilities sample from a categorical distribution, which determines which class the variant will be sampled from. Conditional on the marker variance class sample the effect from the relevant normal distribution or give it a zero effect. To sample from the categorical distribution: 1) create a vector of cumulative probabilities calculated from above ordered by category, and 2) accept the lowest c such that the cumulative probability > u, where u is sample from a uniform distribution $U\left(0,1\right)$.

The proportion of markers in each mixture class $\pi$ follows a $Direchlet~\left( C,c + \alpha\right)$  distribution, where $\textbf{c}$ is a vector of length C that contains the counts of the number of variants in each variance class and $\alpha = \left( 1,1,..,1 \right)$.

The full conditional posterior distribution for  $\sigma _{ \beta }^{2}~$ is a scaled inverse chi-squared distribution with  $\upsilon _{ \beta }=\widetilde{ \nu }_{ \beta }+q$  degrees of freedom and scale parameter $S_{ \beta }^{2}=\frac{\widetilde{ \nu }_{ \beta }\widetilde{S}_{ \beta }^{2}+ \sum_{j=1}^{q}\frac{b_{j}^{2}}{ \gamma _{d_{j}}}}{\widetilde{ \nu }_{ \beta }+q}$  where $\widetilde{ \nu }_{ \beta }~$ and  $\widetilde{S}_{ \beta }^{2}$ are the prior degrees of freedom and scale parameters.

The full conditional posterior distribution for $\sigma_{e}^{2}$ is a scaled inverse chi-squared distribution with $\upsilon_{e}=\widetilde{\nu}_{e}+n$ degrees of freedom and scale parameter $S_{\beta}^{2}=\frac{\widetilde{\nu}_{e}\widetilde{S}_{e}^{2}+sse}{\widetilde{\nu }_{e}+n}$  where $\widetilde{\nu}_{e}$ and $\widetilde{S}_{e}^{2}$ are the prior degrees of freedom and scale parameters and $sse=y^{'}y-b^{'}r^{\ast}-b^{'}X^{'}y$.


## Extensions to summary statistics
The key parameter of interest in the multiple regression model are the marker effects. These can be obtained by solving an equation system similar to:

\begin{align}
b &= \left( X'X +I\frac{\sigma_{e}^{2}}{\sigma_{b}^{2}} \right)^{-1}X'y
\end{align}

In order to solve this equation system individual level data (genotypes $X$ and phenotypes $y$) is required. 
If these are not available, it is possible to reconstruct $X'y$  and $X'X$ from a LD correlation matrix $B$ (from a population matched LD reference panel) and summary statistics: 

\begin{align}
 X'X &= D^{0.5}BD^{0.5} \\
 X'y &= Db_{m} 
\end{align}

where $D_{i}=\frac{1}{\sigma_{b_{i}}^{2}+b_{i}^{2}/n_{i}}$ if the genotypes have been centered to mean 0 or $D_{i}=n_{i}$ if the genotypes have been centered to mean 0 and scaled to unit variance, and $b_{m} = D^{-1}X'y$ is the marginal marker effects obtained from a standard GWAS. The construction of a LD correlation matrix, B, is shown in session 5. 

The summary statistics methods used require the construction of a reference LD correlation matrix. Typically this is done through the use of a fixed 1–10-Mb window approach, as in GCTA-SBLUP or LDpred, which sets LD correlation values outside this window to zero. Zhu and Stephens detail the reasons for using the shrinkage estimator of the LD matrix, which shrinks the off-diagonal entries of the LD correlation matrix towards zero and is required for the implementation based on summary statistics. Experimentation with different types of sparse LD correlation matrices led to the conclusion that the shrinkage estimator was the most stable for SBayesR implementation. Briefly, each element of the reference LD correlation matrix Bij is shrunk by the factor $exp(-\varrho_{ij}/2m)$, where m is taken to be the sample size used to generate the genetic map, $(\varrho _{ij})$ is an estimate of the population-scaled recombination rate between SNPs i and j taken as $\varrho _{ij} = 4N_{\mathrm{e}}c_{ij}$, for Ne the effective population size and $c_{ij}$ the genetic distance between sites i and j in centimorgans as stated in Li and Stephens. LD matrix entries are set to zero if $exp(-\varrho _{ij}/2m)$ is less than a user-chosen cutoff.

Genetic distance between sites is derived from the genetic map files containing interpolated map positions for the CEU population generated from the 1000G OMNI arrays (Data availability). The calculation of the shrunk LD matrix requires the effective population sample size, which we set to be 11,400 (as in Zhu and Stephens), the sample size of the genetic map reference, which corresponds to the 183 individuals from the CEU cohort of the 1000G and the hard threshold on the shrinkage value, which we set to $10^{-3}$. This threshold gave a good balance between computational efficiency and accuracy with, on average, each SNP having a window width of 10.6 Mb (SD=5.6Mb) across the autosomes. The shrunk LD matrix is stored in a sparse matrix format (ignoring matrix elements equal to 0) for efficient SBayesR computation. Currently, the LD matrix construction can only be performed with PLINK hard-call genotypes.


## Fine mapping approaches based on BLR models 
BLR models used for fine-mapping have been specialized in order to focus on the markers that have the largest chance of being causal. The challenge of BLR models is determining which markers have non-zero effect sizes on a trait. The BLR models are fitted using all markers simultaneously, and fine mapping approaches must therefore consider sets of markers in a genomic region rather than individual markers, because the effect of a causative mutation may be distributed across multiple markers. Although markers with non-zero effect sizes may be referred to as causal, it is important to realize that statistical methods alone cannot determine causality.  

For BLR models, several association statistics have been developed to identify which markers or windows of markers can be considered as explaining a substantial or significant proportion of the genetic variance.

The genomic region used for defining the marker set can be defined by  non-overlapping or sliding windows across the genome. The genomic region used in the fine mapping procedure may be determined by a fixed number of markers (e.g. 1000 markers) or by physical (e.g. 1 Mb) or genetic maps (e.g. 1 cM). 

In general a BLR model for fine-mapping can be represented by an indicator variable for each marker (or marker set), with values of 1 for causal and 0 for not, and by organizing these indicators for all markers (or marker set) of interest in vector d. For m markers, there are $2^m$ possible d vectors (hence, $2^m$ possible models), ranging from all values of d equal to 0 for no markers causal to all values equal to 1 for all markers causal. There are several ways to specify the prior probability for a model, such as assuming that variants are independent and equally likely to be causal or assuming a fixed number of causal variants out of the total variants. 

- To what extend mapping power and false discoveries of the BLR models depends on the choice of prior require further investigation. 

- To what extend mapping power and false discoveries of the BLR models depends on the choice of association statistics require further investigation. 

- To what extend mapping power and false discoveries of the BLR models depends on the choice of genomic region require further investigation. 


## Extensions to Multiple Traits and Multiple Components

Bayesian linear regression models have been extended to multiple trait analyses. A multiple trait analysis is a natural choice in quests for understanding and dissecting genetic correlations between traits using genetic markers, e.g., evaluating whether pleiotropy or linkage disequilibrium are at the root of between-trait associations. Furthermore pleiotropy among traits can be utilised to increase accuracy of genomic predictions. 

Multiple trait Bayesian linear regression models can implemented under an restrictive assumption that a locus simultaneously affects all the traits or none of them (Jia and Jannick 2012). This approach has the advantage of simplicity, but the disadvantage that many effects might need to be estimated for loci that have no effect on a trait, and this may erode the accuracy of prediction. This should not be a problem for asymptotically large datasets, as in that case the fitted locus effects should converge to zero for those traits not influenced by that locus. Furthermore, this assumption is not biologically meaningful, especially in multiple trait analyses involving many traits. This assumption of genetic architecture is violated if some loci have no effect on at least one of the traits while having an effect on the remaining traits.   

Therefore a general multiple trait Bayesian linear regression model based on the Bayes C prior have been proposed (Cheng et al. 2018). In this model a locus is allowed to affect any combination of traits, e.g., in a 2-trait analysis, the “restrictive” model only allows two situations, whereas in the general model allow all $2^2=4$ situations. This model is particularly interesting because it provides insight into whether markers affect all, some, or none of the traits addressed.For example, the proportion of markers in each of the (0,0), (1,0), (0,1) and (1,1) categories, where (0,0) means “no effect,” and (1,1) denotes “effect” on both traits. This model becomes computational intensive for a multiple trait analysis of a large number of traits. To partly reduce the computational complexicity it can be implemented such that the multi-variate prior distributions used for marker effects assume the same correlation for all markers. 

A multiple trait Bayesian linear regression model based on the Bayes R prior (Kemper et al. 2022) which can be considered a hybrid model (compared to MT-BLR-STRICT and MT-BLR-FLEX). In this model the prior for the marker effects is a mixture of normal distributions (i.e. Bayes R prior). However these distributions were assumed to be independent between traits but with a specified prior proportion of markers having no effect on any trait. Thus the "learning" between traits is primarily from the marker-trait indicator variable which is a slightly more flexible approach.

The above mentioned MT-BLR can also be implemented using the Bayes N and Bayes A priors (Fernando and Gianola).

In summary MT-BLR models provide a flexible approach for the detection of pleiotropic loci which can be utilised to increase accuracy of genomic predictions. However, the impact of prior assumptions on marker effects across traits requires further investigation. Also, an important limitation of all methods that attempt to use genotype–phenotype associations to detect pleiotropic loci is that they cannot differentiate the presence of a pleiotropic locus from the presence of two closely linked single-trait loci, depending on the extent of LD in the region. Furthermore issues with inferences about genetic (co) variance based on BLR models can misrepresent the true genetic parameters if the causal loci are not genotyped because of incomplete LD between markers and causal loci and among causal loci (de los Campos et al., Gianola et al.).


### Posterior sampling distributions for multiple trait BLR model based on the Bayes C prior

A multiple trait BLR (MT-BLR) model estimate joint marker effect accounting for LD and borrow information across traits based on the following equation (two-trait example):

\begin{align}
b= \left[ \begin{matrix}
b_{1}\\
b_{2}\\
\end{matrix}
 \right] = \left(  \left[ \begin{matrix}
X_{1}^{'}X_{1}^{}  &  0\\
0  &  X_{2}^{'}X_{2}^{}\\
\end{matrix}
\right] ~ +I\otimes B^{-1}E \right) ^{-1} \left[ \begin{matrix}
X_{1}^{'}y_{1}^{}\\
X_{2}^{'}y_{2}^{}\\
\end{matrix}
\right] 
\end{align}

- $b_1$ and $b_2$ are the marker effects for trait 1 and 2
- $y_1$ and $y_2$ are the vectors of phenotypes for trait 1 and 2
- $X_1$ and $X_2$ are the matrices of genotypes for trait 1 and 2
- $X_{1}^{'}X_{1}^{}$ and $X_{2}^{'}X_{2}^{}$ are the LD t-erms which may differ across ancestries

- $B$ is the genetic co-variance matrix between traits
\begin{align}
B= \left[ \begin{matrix}
\sigma_{b_{1}}^{2}  &   \sigma_{b_{12}}^{2}\\
\sigma_{b_{21}}^{2}  &   \sigma_{b_{2}}^{2}\\
\end{matrix}
\right]
\end{align}

- $E$ is the residual co-variance matrix between traits
\begin{align}
E= \left[ \begin{matrix}
\sigma _{e_{1}}^{2}  &   \sigma _{e_{12}}^{2}\\
\sigma _{e_{21}}^{2}  &   \sigma _{e_{2}}^{2}\\
\end{matrix}
\right] 
\end{align}
 
- the shrinkage term, $B^{-1}E$, that depends on both prior distribution of the marker effects (e.g. Bayes C) and the model selection procedure (e.g. marker-trait configuration) 
 

The covariance matrix $\textbf{B}$ for the marker effect is a priori assumed to follow an inverse Wishart distribution $IW(\widetilde{S}_b^{2},\widetilde{\nu}_b)$ where $\widetilde{\nu}_b$ and  $\widetilde{S}_b^{2}$ (t by t matrix) are the prior degrees of freedom and scale parameters. 

The full conditional distribution for $\textbf{B}$ the covariance matrix for the jth marker is also an inverse Wishart distribution of the form:

$IW(\widetilde{S}_b^{2} + bb' ,\widetilde{\nu}_b+m)$, where $\textbf{b}$ is a matrix (m x t) of sampled marker effects

or

$IW(\widetilde{S}_{\beta}^{2} + b_{j}b_{j}^{'}, \widetilde{\nu}_{\beta}+1)$, where  $b_{j}$ is a vector (t x 1) of sampled marker effects.

Difference between the two sampling distributions is that the former is the same for all markers whereas the latter is more flexible as it allows the markers to their own covariance matrix.

The covariance matrix $\textbf{E}$ for the residual effects is a priori assumed to follow an inverse Wishart distribution $IW(\widetilde{S}_{e}^{2}, \widetilde{\nu}_{e})$ where  $\widetilde{\nu}_{e}$ and $\widetilde{S}_{e}^{2}$ (t by t matrix) are the prior degrees of freedom and scale parameters.

The full conditional distributions for the covariance matrix \textbf{E} for residuals is also an inverse Wishart distribution of the form:

IW($\widetilde{S}_{e}^{2} + ee'$ , $~\widetilde{ \nu }_{e}+m$), where \textbf{e} is a matrix (n x t) of residual effects

The full conditional distribution for  $b_{j}~$ can be be written as:

\begin{align}
f_{j} \left( b_{j} \vert  \theta _{-j},d_{j},\widetilde{y} \right) \propto N \left( C_{j}^{-1}r_{j},C_{j}^{-1} \right)   
\end{align}

where $C=D_{j}^{'}E^{-1}D_{j}x_{j}^{'}x_{j}+G^{-1}$  and  $r_{j}=x_{j}^{'}\widetilde{y}E^{-1}D_{j}=\widetilde{b}_{j}x_{j}^{'}x_{j}E^{-1}D_{j}$ 

where  $D_{j}$  is a diagonal matrix whose kth diagonal entry is an indicator variable indicating whether the marker jth effect for trait k is zero or nonzero.

The marginal full conditional probability for the indicator variable of  $d_{j}$  is:

\begin{align}
f_{j} \left( d_{j}=c \vert  \theta _{-j},\widetilde{y} \right) =\frac{f_{j} \left( \widetilde{y} \vert d_{j}=c, \theta _{-j} \right) f \left( d_{j} \vert  \pi _{c} \right) }{ \sum _{k=1}^{C}f_{j} \left( \widetilde{y} \vert d_{j}=k, \theta _{-j} \right) f \left( d_{j} \vert  \pi _{k} \right) }
\end{align}

where  $f_{j} \left( \widetilde{y} \vert d_{j}=c, \theta _{-j} \right) = \vert C_{j}^{-1} \vert ^{0.5}\exp  \left[ \frac{1}{2}r_{j}^{'}C_{j}^{-1}r_{j}^{} \right]$

In the most general case, any marker effect might be zero for any possible combination of t traits resulting in $2^t$ possible combinations of $d_{j}$: For example, in a t=2 trait model, there are $2^t=4$  combinations for $d_{j}$: (0,0), (0,1), (1,0), (1,1).  Suppose, in general, we use numerical labels $1$,$2$,...,$l$  for the $2^t$ possible outcomes for $d_{j}$  then the prior for $d_{j}$ is a categorical distribution.



# Bayesion Linear Regression Model for Individual Effects

## Statistical model and model parameters

We begin with a description of the general two-trait model since the simpler models are special cases. The general two-trait model partitions the total phenotypic variance into a component explained by genetic marker effects, $g$, and a component due to residual effects that are not associated with markers, $e$. Additionally, the component due to genetic markers $g$ is subdivided into contributions from $C$ marker sets $g_{i}$, $i=1,\ldots ,C$. 

The model for trait 1 is assumed to be

\begin{equation}
y_{1}=\mu_{1}+\sum\nolimits_{i=1}^{C}{g_1}_{i}+e_{1},  \label{em}
\end{equation}

and similar for trait 2,

\begin{equation}
y_{2}=\mu_{2}+\sum\nolimits_{i=1}^{C}{g_2}_{i}+e_{2},  \label{em}
\end{equation}

In these expressions, the $\mu$'s are scalar means for each trait, the $g^{\prime}$s are contributions to genetic effects from each of $C$ marker sets that can be associated with genetic marker information (genomic effects or genomic values), the $e^{\prime }s$ represent a
residual component that cannot be captured by regression on markers.

The genomic value for marker set $k$ is defined as the sum of the effects of all the markers in marker set $k$,

\begin{align}
g_1k &= \sum\nolimits_{i=1}^{p_{k}}{w}_{ijk}b_{1ik}  \notag \\
&= {w}_{jk}^{\prime }b_{1k},  \label{biv2}
\end{align}

with an equivalent expression for trait 2. In (\ref{biv2}), $p_{k}$ is the number of markers in marker set $k$, the scalar ${w}_{ijk}$ is the observed (centered and scaled) label for marker $i$ in individual $j$ for marker set $k$, and $b_{1ik}$ is the effect of marker $i$ for marker set $k$ of trait 1. 

The $n$ by $1$ vectors of genomic effects for trait 1 and trait 2 for marker set $k$ are $g_{1k}=W_{k}b_{1k}$ and $g_{2k}=W_{k}b_{2k}$, respectively, where $W_{k}=\left\{w_{ijk}\right\}$ is the observed $n$ by $p_{k}$ matrix of marker genotypes of marker set $k$. The joint distribution of vectors $b_{1k}$ and $b_{2k}$ is assumed to be


\begin{equation}
\left[
\begin{array}{c}
b_{1k} \\
b_{2k}
\end{array}
\right] \sim N\left( \left[
\begin{array}{c}
0 \\
0
\end{array}
\right] ,\left[
\begin{array}{cc}
I\sigma _{b_{1k}}^{2} & I\sigma _{b_{1k}b_{2k}} \\
I\sigma _{b_{1k}b_{2k}} & I\sigma _{b_{2k}}^{2}
\end{array}
\right] \right)  \label{biv1}
\end{equation}

where the $I^{\prime }$s represent $p_{k}$ by $p_{k}$ identity matrices, $\sigma_{b_{1k}}^{2}$ $\left(\sigma_{b_{2k}}^{2}\right)$ is the prior variance of marker effects for trait 1 (trait 2) in marker set $k$, and $\sigma_{b_{1k}b_{2k}}$ is their prior covariance.

Due to the centering the rank of $W_{k}$ is $n-1$. It follows from these assumptions and from standard properties of the multivariate normal distribution that the model for the joint distribution of genomic values in trait 1 and 2 is the singular multinormal ($SN$) distribution,

\begin{equation}
\left[
\begin{array}{c}
g_{1k} \\
g_{2k}
\end{array}
\right] \sim SN\left( \left[
\begin{array}{c}
0 \\
0
\end{array}
\right] ,\left[
\begin{array}{ll}
G_{k}\sigma_{g_{1k}}^{2} & G_{k}\sigma_{g_{1k}g_{fk}} \\
G_{k}\sigma_{g_{1k}g_{2k}} & G_{k}\sigma_{g_{2k}}^{2}
\end{array}
\right] \right) ,\quad k=1,\ldots ,C.  \label{biv3}
\end{equation}

Above, $\sigma_{g_{1k}}^{2}$ is the genomic variance in trait 1 for marker set $k$, $\sigma_{g_{2k}}^{2}$ is the genomic variance in trait 2 for marker set $k$, $\sigma_{g_{1k}g_{2k}}$ is the genomic covariance between trait 1 and 2 for marker set $k$, $G_{k}=\frac{1}{p_{k}} W_{k}W_{k}^{\prime}$ is the genomic relationship matrix of rank $n-1$ for marker set $k$, and $SN$ denotes the singular normal distribution (details in the Supplementary Methods). A more compact notation for (\ref{biv3}) is

\begin{equation}
g_{k}=\left( g_{1k}^{\prime },g_{2k}^{\prime }\right) ^{\prime }\sim
SN\left( 0,G_{k}\otimes Vg_{k}\right)  \label{biv31}
\end{equation}

where

\begin{equation}
{V_g}_k=\left[
\begin{array}{ll}
\sigma _{g_{1k}}^{2} & \sigma _{g_{1k}g_{2k}} \\
\sigma _{g_{1k}g_{2k}} & \sigma _{g_{2k}}^{2}
\end{array}
\right] .  \label{vgi}
\end{equation}

Similarly, collecting the $e^{\prime }s$ in vectors with $n$ elements for each trait joint distribution is assumed to be

\begin{equation}
\left[
\begin{array}{c}
e_{1} \\
e_{2}
\end{array}
\right] \sim N\left( \left[
\begin{array}{c}
0 \\
0
\end{array}
\right] ,\left[
\begin{array}{ll}
I\sigma_{e_{1}}^{2} & I\sigma_{e_{1}e_{2}} \\
I\sigma_{e_{1}e_{2}} & I\sigma_{e_{2}}^{2}
\end{array}
\right] \right) ,  \label{eh}
\end{equation}

The identity matrix $I$ is of order $n$ by $n$ in (\ref{eh})\ . The phenotypes for trait 1 and 2, $y_{1}$ and $y_{2}$, each with $n$ elements, are conditionally normally distributed, given $g$, and $e$, and take the form

\label{y}
\begin{eqnarray}
y_{1}|\mu_{1},g_{1} &\sim & N\left(1\mu_{1}+\sum\nolimits_{i=1}^{C}g_{1i},I\sigma_{e_1}^{2}\right) ,
\label{ym} \\
y_{2}|\mu_{2},g_{2} &\sim & 
N\left( 1\mu_{2}+\sum\nolimits_{i=1}^{C}g_{2i},I\sigma_{e_2}^{2}\right) ,
\label{yf}
\end{eqnarray}

where $\sigma_{e1}^{2}$ ($\sigma_{e2}^{2}$) is the residual variance for the trait 1 (trait 2). 


## Extensions to multiple components
The model specified quantify the relative contribution from each of the marker sets by fitting separate variance components for each of those marker sets. Here we describe the basis for this partitioning. The point of departure is to assume that the $2n_{t}$ by $1$ vector of genomic effects $g=\left(g_{1}^{\prime},g_{2}^{\prime}\right)^{\prime}$ is equal to the sum of the vectors of genomic effects belonging to $C$ marker set components. Then 

\begin{eqnarray}
g =\sum_{i=1}^{C}g_{i},Var\left( g|W\right) =\sum_{i=1}^{C}Var\left( g_{i}|W_{i}\right)
=\sum_{i=1}^{C}\frac{1}{p_{i}}W_{i}W_{i}^{\prime}\otimes {V_g}_i
\end{eqnarray}

where $g_{i}$ is the $2n$ by $1$ vector of genomic values for trait 1 and 2, associated with component $i$, $i=1,\ldots,C$, $p_{i}$ is the number of marker genotypes in component $i$, and ${V_g}_i$ is the $2$ by $2$ genomic covariance matrix for marker set $i$. This holds under the model assumption that the components of the vector of marker effects $b=\left( b_{1}^{\prime },\ldots ,b_{i}^{\prime },\ldots b_{C}^{\prime }\right) ^{\prime }$ are realisations from $b_{i}\sim N\left( 0,I\otimes {V_b}_i\right)$, with $Cov\left(b_{i},b_{j}^{\prime }\right) =0$, $i,j=1,\ldots ,C;i\neq j$. Then $Cov\left(g_{i},g_{j}^{\prime }|W\right)=Cov\left( W_{i}b_{i},b_{j}^{\prime}W_{j}^{\prime }\right)=0$. In these expressions, $b_{i}=\left({b_1}_{i}^{\prime },{b_2}_{i}^{\prime }\right) ^{\prime}$ is the vector of marker effects of marker set component $i$, and

\begin{equation}
{V_b}_i=\left[\begin{array}{cc}
{\sigma_{b_1}}_{i}^{2} & \sigma_{{b_1}_{i} {b_2}_{i}} \\
\sigma_{{b_1}_{i}{b_2}_{i}} & {\sigma_{b_2}}_{i}^{2}
\end{array}\right].
\end{equation}

Two simple two-trait models are implemented that partition the total genomic variance into multiple component defined by marker sets. Details of the prior distributions of the parameters of the Bayesian models and of the Markov chain Monte Carlo algorithm can be found in the Supplementary Methods.


### Inferences
For single trait analysis, inferences are reported in terms of ratios for the ith marker sets defined as:

\begin{equation}
h_{g_i}^{2}=\frac{\sigma_{{g}_{i}}^{2}}{\sigma_{{g}}^{2}},\quad i=1,\ldots ,C.
\end{equation}

with $\sigma_{{g}}^{2}=Var\left(\sum\nolimits_{i=1}^{C}g_{i}\right)$. When the elements of $W$ are scaled so that the average of the diagonal elements of $WW^{\prime}$ is equal to $1$, this ratio quantifies the proportion of total genomic variance, captured by genetic marker information associated with marker set $i$. 
The ratio involving marker effects from all the marker sets for trait 1 is

\begin{equation}
h_{g_1}^{2}=\frac{\sigma_{g_1}^{2}}{\sigma_{g_1}^{2}+\sigma_{e_1}^{2}}  
\end{equation}

with a similar expression for trait 2.

We also report within marker set genomic correlations

\begin{equation}
r_{g_{i}}=\frac{\sigma_{g_{i_1}g_{i_2}}}{\sigma_{{g}_{i_1}}\sigma _{{g}_{i_2}}}
,\quad i=1,\ldots ,C,  
\end{equation}

and the total genomic correlation between traits, defined as

\begin{equation}
r_{g_{i}}=\frac{Cov\left(
\sum\nolimits_{i=1}^{C}g_{1_{i}},\sum\nolimits_{i=1}^{C}g_{2_{i}}\right)}{\sqrt{
\sigma _{{g}_{1}}^{2}\sigma _{{g}_{2}}^{2}}}
\end{equation}

The absence of symmetry of posterior distributions is well captured by the Bayesian McMC methods. In a situation like this, summarizing results in the form of posterior means and standard deviations would be misleading. In the presence of asymmetry, posterior means are poor indicators of points of high probability mass. We therefore summarize inferences in terms of posterior modes and posterior intervals.

## Estimation of model parameters

The models are implemented using empirical Bayesian methods. The models were implemented using a Bayesian, Markov chain Monte Carlo approach, whereby the hyperparameters of the dispersion parameters of the Bayesian model were estimated by maximum likelihood, and conditional on these, the model was fitted using Markov chain Monte Carlo. To illustrate, consider determining a value of the scale parameter of an inverse chi square distribution, which is the prior assumed for the variance components. This is achieved by equating the mode of the inverse chi square prior density, for a fixed value of the degrees of freedom, to the maximum likelihood estimate and solving for the scale parameter. The decision to huse this approach rather than classical likelihood is based on the need to obtain measures of uncertainty that account as much as possible for the limited amount of information in the data without resorting to large sample theory, which is inherent in classical likelihood. This is relevant if the data are of limited size and results in marginal posterior distributions that are expected to be asymmetric, particularly in the case of the more complex, general two-trait model. 

A number of technical details are presented in this section. These include a formal derivation of the probability model for the variance between traits and details of a spectral decomposition that plays an important computational role in the Markov chain Monte Carlo strategy implemented, as well as a full description of the Bayesian models. 

### The spectral decomposition 
The Bayesian implementation of the model is facilitated making use of the following spectral decomposition performed within each chromosome segment (we drop subscripts that refer to marker sets to avoid clotting the notation). Let


\begin{equation}
g_{1}=Wb_{1} \\
g_{2}=Wb_{2}
\end{equation}

where $W$ is of order $n\times p$ and $g_{m}$ is of order $n\times 1$. The spectral decomposition of $WW^{\prime}$ (for each marker set) is

\begin{eqnarray}
WW^{\prime } &= U\Delta U^{\prime} &= \sum\nolimits_{i=1}^{n_{t}}\lambda_{i}U_{i}U_{i}^{^{\prime }},
\end{eqnarray}

where $U=\left[U_{1},U_{2},\ldots ,U_{n_{t}}\right]$, of order $n\times n$ is the matrix of eigenvectors of $WW^{\prime}$, $U_{j}$ is the jth column (dimension $n\times 1$), and $\Delta$ is a diagonal matrix with elements equal to the eigenvalues $\lambda_{1},\lambda_{2},\ldots,\lambda_{n_{t}}$ associated to the $n_{t}$ eigenvectors. Since $WW^{\prime}$ is positive semidefinite the eigenvalues are $\lambda_{i}\geq 0$, $i=1,2,\ldots ,n$. The eigen vectors satisfy $U^{\prime}U=UU^{\prime}=I$.


Define the $n\times n$ matrix $G=\frac{1}{p}WW^{\prime}=$ $\frac{1}{p}WW^{\prime }$ and write this as
\begin{eqnarray*}
G &= \frac{1}{p}WW^{\prime } \\
&= \frac{1}{p}U\Delta U^{\prime } \\
&= UDU^{\prime }
\end{eqnarray*}
where
\begin{equation*}
D=\frac{1}{p}\Delta .
\end{equation*}

Matrix $G$ (peculiar to each marker set) is singular and the diagonal matrix $D$ (also peculiar to each marker set) contains only $n-1$ positive eigenvalues.

\subsubsection{The case of singular $G$ }

Due to the singularity of $G$, $\left[ g|W,\sigma_{g}^{2}\right]$ does not follow a multivariate normal distribution but a singular normal distribution instead. The singular normal density is

\begin{equation}
p\left( g_{1}|W,\sigma_{g1}^{2}\right) =\frac{1}{\left(
2\pi \right) ^{\frac{n-1}{2}}\left( \lambda_{1}\sigma_{g_{1}}^{2}\ldots \lambda_{n-1}\sigma_{g_{1}}^{2}\right) ^{\frac{1}{2}}}\exp \left( -\frac{g_{1}^{\prime }G^{-}g_{1}}{
2\sigma_{g_{1}}^{2}}\right)  \label{e7}
\end{equation}

where the $n-1$ $\lambda^{\prime}s$ are the non-zero eigenvalues of $G$ and $G^{-}$ is any generalised inverse of $G$. One choice choice of generalised inverse of $G$ is

\begin{equation}
G^{-}=UD^{-}U^{\prime }
\end{equation}

where $U$ and $D$ are of order $n_{t}$ by $n_{t}$

\begin{equation*}
D^{-}=\frac{1}{p}\left[
\begin{array}{ccccc}
\frac{1}{\lambda_{1}} & 0 & \ldots & \ldots & 0 \\
0 & \ddots & \ldots & \ldots & 0 \\
\vdots & \vdots & \frac{1}{\lambda_{n-1}} & \ldots & 0 \\
\vdots & \vdots & \ldots & \ddots & 0 \\
0 & 0 & 0 & 0 & 0
\end{array}
\right] =\frac{1}{p}\left[
\begin{array}{cc}
D_{1}^{-1} & 0 \\
0^{\prime } & 0
\end{array}
\right]
\end{equation*}

Above, $D_{1}=diag\left( \lambda_{i}\right)_{i=1}^{n-1}$, a diagonal matrix of dimension $n-1$ by $n-1$ that contains the nonzero eigenvalues $\lambda _{i}$. The remaining elements of $D^{-}$ are all equal to zero.

\subsubsection{A probabilistically equivalent reparameterisation of the random regression model}

We describe a reparameterisation of the original two-trait model that simplifies the Markov chain Monte Carlo computations.

For each marker set (omitting the subscripts) define the row vector random variable $\alpha^{\prime}=\left(\alpha_{1}^{\prime},\alpha_{2}^{\prime}\right)$ of dimension $1$ by $2n$, with vectors $\alpha_{1}$ and $\alpha_{2}$, each of dimension $n$ by $1$ associated with trait 1 and 2, with distribution

\begin{equation}
\left(
\begin{array}{c}
\alpha _{1} \\
\alpha _{2}
\end{array}
\right) \sim SN\left( \left[
\begin{array}{c}
0 \\
0
\end{array}
\right] ,\left[
\begin{array}{cc}
D\sigma_{g_{1}}^{2} & D\sigma_{g_{1}g_{2}} \\
D\sigma_{g_{1}g_{2}} & D\sigma_{g_{2}}^{2}
\end{array}
\right] \right)  \label{alf1}
\end{equation}

where $D$ is a diagonal matrix (associated with a particular marker set) of dimension $n\times n$, with has eigenvalues $\lambda_{i}$, $i=1,\ldots ,n$ as diagonal elements, of which the first $n-1$ are positive and the rest are equal to zero. We define for each marker set
\begin{equation}
V_{g}=\left[
\begin{array}{cc}
\sigma_{g_{1}}^{2} & \sigma_{g_{1}g_{2}} \\
\sigma_{g_{1}g_{2}} & \sigma_{g_{1}}^{2}
\end{array}
\right] .  \label{alf2}
\end{equation}

In a particular marker set, the random variables
\begin{equation}
\left(
\begin{array}{c}
U\alpha_{1} \\
U\alpha_{2}
\end{array}
\right) \sim SN\left( \left[
\begin{array}{c}
0 \\
0
\end{array}
\right] ,\left[
\begin{array}{cc}
UDU^{\prime}\sigma_{g_{1}}^{2} & UDU^{\prime}\sigma_{g_{1}g_{2}} \\
UDU^{\prime}\sigma_{g_{1}g_{2}} & UDU^{\prime}\sigma_{g_{2}}^{2}
\end{array}
\right] \right)  \label{ualfa}
\end{equation}
and
\begin{equation}
\left(
\begin{array}{c}
g_{1} \\
g_{2}
\end{array}
\right) \sim SN\left( \left[
\begin{array}{c}
0 \\
0
\end{array}
\right] ,\left[
\begin{array}{cc}
\frac{1}{p}WW^{\prime}\sigma_{g_{1}}^{2} & \frac{1}{p}
WW^{\prime }\sigma_{g_{1}g_{2}} \\
\frac{1}{p}WW^{\prime}\sigma_{g_{1}g_{2}} & \frac{1}{p}
WW^{\prime }\sigma_{g_{2}}^{2}
\end{array}
\right] \right)  \label{alfazg}
\end{equation}

have the same distribution since $UDU^{\prime }=\frac{1}{p}WW^{\prime }$. Here $p$ is the number of genetic markers in the marker set.
Therefore the structure can be written as

\label{yt}
\begin{eqnarray}
y_{1}|\mu_{1},\sum\nolimits_{i=1}^{C}\alpha_{1i} &\sim
&N \left( 1\mu_{1}+\sum\nolimits_{i=1}^{C}U_{i}\alpha_{1i}, I\sigma_{1e}^{2}\right),  \label{ytm} \\
y_{2}|\mu_{2},\sum\nolimits_{i=1}^{C}\alpha_{2i} &\sim
&N \left( 1\mu_{2}+\sum\nolimits_{i=1}^{C}U_{i}\alpha_{2i}, I\sigma_{2e}^{2}\right) \quad i=1,\ldots ,C.  \label{ytf}
\end{eqnarray}


\subsubsection{Prior distributions}

The prior distribution of the $\mu ^{\prime }s$ is $N\left( 0,10^{5}\right)$. The prior distributions of $V_{g}$ and $V_{h}$ are scaled inverted Wishart with degrees of freedom set equal to $2.5$ and scale parameters $P_{g}$ and $P_{h}$, respectively. The degrees of freedom generate a proper distribution with overdispersed values. The prior distributions of $\sigma_{r_{m}}^{2}$ and $\sigma_{r_{f}}^{2}$ are scale inverted chi square densities. The degrees of freedom of these densities are set equal to $1.0$ which leads to vague prior information. For example, when the scale is equal to $0.1$, the modal value of the prior distribution is $0.03$ and the prior probability that the variance component is smaller than this value is $56$. The prior probability that the variance component is between $0.03$ and $0.3$ is $29$.

The scale parameters of all these distributions are estimated using maximum likelihood. This involved obtaining maximum likelihood estimates of the two-trait model (for Models $H$ and $G$, each sex considered as one trait), and then equating these estimates to the mode of the relevant prior distribution, written as a function of the scale parameter. In the case of the general two-trait model, single-trait likelihoods were fitted instead. The off-diagonal elements of the scale parameter of inverse Wishart distributions were set equal to zero.

\subsubsection{Fully conditional posterior distributions}

The fully conditional posterior distributions of a parameter $\theta$ is denoted $\left[\theta|All,z\right]$ where $All$ denotes all the parameters of the model except $\theta$. Here we sketch the form of these densities.


\paragraph{Updating the $\alpha^{\prime}s$}

with subscripts $1$ for trait 1, $2$ for trait 2 and $k$ for marker set, from the bivariate normal distribution 

\begin{equation*}
\left[ \left.
\begin{array}{c}
\alpha _{ki1} \\
\alpha _{ki2}
\end{array}
\right\vert All,y\right] \sim N\left( \left[
\begin{array}{c}
\widehat{\alpha }_{ki1} \\
\widehat{\alpha }_{ki2}
\end{array}
\right] ,\left[ I+\frac{\sigma_{e}^{2}}{\lambda _{ki}}\left(
\begin{array}{cc}
\sigma _{g_{1}}^{2} & \sigma _{g_{1}g_{2}} \\
\sigma _{g_{1}g_{2}} & \sigma _{g_{2}}^{2}
\end{array}
\right) _{k}^{-1}\right] ^{-1}\sigma _{e}^{2}\right) ,\quad i=1,2,\ldots
,n-1,
\end{equation*}
where
\begin{equation*}
\left( I+\lambda _{ki}^{-1}V_{g_{k}}^{-1}\sigma _{e}^{2}\right) \widehat{
\alpha }_{is}=\left[
\begin{array}{c}
U_{ki}^{\prime }\left( y_{1}-1\mu_{1}-\sum\nolimits_{i\neq k}U_{i}\alpha
h_{i1}\right)  \\
U_{ki}^{\prime }\left( y_{2}-1\mu_{2}-\sum\nolimits_{i\neq k}U_{i}\alpha
_{i2}\right)
\end{array}
\right] ,\qquad \alpha _{is}^{\prime }=\left( \alpha _{i1},\alpha
_{i2}\right).
\end{equation*}

The strategy updates jointly the $\alpha^{\prime}s$ for both traits from a given marker set, conditional on the $\alpha^{\prime}s$ from the remaining marker sets.

\paragraph{Updating $\mu_{1}$ and $\mu_{2}$}

The prior distribution of both scalars $\mu_{1}$ and $\mu_{2}$ are the normal process $N\left(0,10^{5}\right)$. The fully conditional for $\mu_{1}$ is proportional to 

\begin{equation*}
\left[\mu_{1}|All,x\right] \sim N\left(\widehat{\mu}_{1},\left(1^{\prime}1+k_{\mu_{1}}\right) ^{-1}\sigma_{e}^{2}\right)
\end{equation*}

where

\begin{equation*}
\left( 1^{\prime }1+k_{\mu_{1}}\right) \widehat{\mu}_{1}=1^{\prime }\left(y_{m}-U\alpha _{m}-T\gamma_{1}\right), \qquad k_{\mu_{1}}=\frac{\sigma_{e}^{2}}{10^{5}}.
\end{equation*}

A similar expression holds for $\mu_{2}$.


\paragraph{Updating $V_{g}$}

For each of the marker sets, the update involves drawing samples from scaled inverse Wishart distributions. The fully conditional of the $2\times 2$ matrix $V_{g}$ is proportional to

\begin{equation}
\left[\alpha_{m},\alpha_{f}|V_{g},D\right] \left[ V_{g}|\nu _{g},P_{g}\right].  
\end{equation}

The second term is inverse Wishart the prior distribution of $V_{g}$, $IW\left(\nu_{g},P_{g}\right)$ with density 
\begin{equation} p\left( V_{g}|\nu _{g},P_{g}\right) \propto \left\vert Vg\right\vert ^{-
\frac{1}{2}\left( v_{g}+3\right) }\exp \left[ -\frac{1}{2}tr\left(
V_{g}^{-1}P_{g}\right) \right]  \label{priorvg}
\end{equation}

where the hyperparameters $\nu_{g}$ and $P_{g}$ are the degrees of freedom and the scale, respectively. The modal value of this distribution is given by $P_{g}/(\nu_{g}+p+1)$, where in our case, $p=2.$ On defining (see \citealp{sorensenandgianola2002}, page 574)

\begin{equation*}
S_{g}=\left[
\begin{array}{cc}
\alpha_{1}^{\prime }D^{-1}\alpha_{1} & \alpha_{1}^{\prime }D^{-1}\alpha_{2} \\
\alpha_{2}^{\prime }D^{-1}\alpha_{1} & \alpha_{2}^{\prime }D^{-1}\alpha_{2}
\end{array}\right],
\end{equation*}

the density of the fully conditional posterior distribution of $V_{g}$ is


\begin{eqnarray}
p\left( V_{g}|All,y\right) &\propto & \left\vert V_{g}\right\vert ^{-\frac{k}{
2}}\left\vert Vg\right\vert ^{-\frac{1}{2}\left( v_{g}+3\right) }\exp \left[
-\frac{1}{2}tr\left( V_{g}^{-1}P_{g}\right) \right] \exp \left[ -\frac{1}{2}
tr\left( V_{g}^{-1}S_{g}\right) \right]  \notag \\
&= & \left\vert Vg\right\vert ^{-\frac{1}{2}\left( k+v_{g}+3\right) }\exp 
\left[ -\frac{1}{2}tr\left[ V_{g}^{-1}\left( S_{g}+P_{g}\right) \right]
\right]  \label{iw1}
\end{eqnarray}

where $k=n-1$, which is in the form of an inverse Wishart distribution of dimension $2$, $k+v_{g}$ degrees of freedom and scale matrix $\left( S_{g}+P_{g}\right)$.

\paragraph{Updating $V_{e}$}

The update here is similar and it involves again drawing samples from scaled inverse Wishart distributions. The fully conditional posterior distribution of the $2\times 2$ matrix $V_{e}$ defined in (\ref{vh}) is proportional to

\begin{equation}
\left[ e_{1},e_{2}|V_{e},E\right] \left[ V_{e}|\nu_{e},P_{e}
\right]  \label{fcvh1}
\end{equation}

where the second term is the prior distribution of $V_{e}$ with hyperparameters $\nu_{e}$ and $P_{e}$ of the form

\begin{equation}
p\left( V_{e}|\nu_{e},P_{e}\right) \propto \left\vert V_{e}\right\vert ^{-
\frac{1}{2}\left( v_{e}+3\right) }\exp \left[ -\frac{1}{2}tr\left(
V_{e}^{-1}P_{e}\right) \right]  \label{priorve}
\end{equation}
symbolised $IW\left( \nu_{e},P_{e}\right)$. 


The density is

\begin{equation*}
p\left( V_{e}|All,y\right) \propto \left\vert V_{e}\right\vert ^{-\frac{1}{2}
\left( n+v_{e}+3\right) }\exp \left[ -\frac{1}{2}tr\left[
V_{e}^{-1}\left( S_{e}+P_{e}\right) \right] \right],
\end{equation*}

an inverse Wishart distribution of dimension $2$, $n+v_{e}$ degrees of freedom and scale matrix $\left(S_{e}+P_{e}\right)$, where

\begin{equation*}
S_{h}=\left[
\begin{array}{cc}
\gamma_{m}^{\prime }E^{-1}\gamma_{m} & \gamma_{m}^{\prime }E^{-1}\gamma
_{f} \\
\gamma_{f}^{\prime }E^{-1}\gamma_{m} & \gamma_{f}^{\prime }E^{-1}\gamma
_{f}
\end{array}
\right].
\end{equation*}

