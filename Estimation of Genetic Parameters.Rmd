---
title: "Estimation of Genetic Parameters"
author: "Izel Fourie Sørensen, Palle Duun Rohde & Peter Sørensen"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    citation_package: natbib
    number_sections: yes
    includes:
      in_header: preamble.tex
  html_document:
    number_sections: yes
    includes:
      in_header: mathjax_header.html
  word_document: default
bibliography: [qg2021.bib]
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(19)
```


# Introduction
The estimation of genetic parameters is an important issue in human genetics. First of all, estimating additive genetic and possible non-additive genetic variances contributes to a better understanding of genetic mechanisms.  Secondly, estimates of genetic and phenotypic variances and covariances are essential for the prediction of genetic predisposition. Parameters that are of interest are heritability, genetic and phenotypic correlation and repeatability, and those are computed as functions of the variance components. Genetic parameters are estimated using information on phenotypes and genetic relationships for individuals in the study population. In this section we will illustrate how different phenotypic sources and genetic relationships are used for estimating genetic parameters.

## Genetic model
The phenotype for a quantitative trait is the sum of both genetic and environmental factors. In general the total genetic effect for an individual is the sum of both additive and non-additive effects: 

\begin{align}
	y &=\mu+a+d+e
\end{align}

where  $\mu$ is the population mean, $a$ is the additive genetic effect, $d$ is the dominance effect, and $e$ is the environmental deviation (or residual) not explained by the genetic effects in the model. Only the additive genetic effects are passed on to the child. In contrast non-additive genetic effects (dominance and epistasis) are degraded by recombination and are not inherited, even though they may be important for the individual's phenotype. Here we only consider the additive and dominance effects. We assume that the genetic effects (i.e., a, and d), and the residual term, e, are independent and normally distributed:

\begin{align}
a \sim N(0,\sigma^2_{a}), \notag \\
d \sim N(0,\sigma^2_{d}), \notag \\
e \sim N(0,\sigma^2_{e}), \notag 
\end{align}

where $\sigma^2_{a}$ is the additive genetic variance, $\sigma^2_{d}$ is the dominance variance, and $\sigma^2_{e}$ is the residual variance. This means that the observed phenotype is also normally distributed $y \sim N( \mu,\sigma^2_{y})$ with the overall phenotypic variance:

\begin{align}
\sigma^2_{y} &=  \sigma^2_{a} + \sigma^2_{d} + \sigma^2_{e} \notag
\end{align}


## Genetic parameters
Heritability and genetic correlation are the key genetic parameters used in estimation of genetic predisposition. They are defined in terms of the variance components ($\sigma^2_{a}$ and $\sigma^2_{e}$) presented in the previous section.    
    
__Heritability__ estimates the degree of variation in a phenotypic trait in a population that is due to genetic variation between individuals in that population. It measures how much of the variation of a trait can be attributed to variation of genetic factors, as opposed to variation of environmental factors. 
The narrow sense heritability is the ratio of additive genetic variance ($\sigma^2_{a}$) to the overall phenotypic variance:
\begin{align}
h^2 &= \sigma^2_{a}/(\sigma^2_a+\sigma^2_d+\sigma^2_e)
\end{align}
and the broad sense heritability is the ratio of overall genetic variance ($\sigma^2_{g}=\sigma^2_{a}+\sigma^2_{d}$) to the overall phenotypic variance:
\begin{align}
H^2 &= (\sigma^2_{a}+\sigma^2_{d})/(\sigma^2_a+\sigma^2_d+\sigma^2_e)
\end{align}

A heritability of 0 implies that no genetic effects influence the observed variation in the trait, while a heritability of 1 implies that all of the variation in the trait is explained by the genetic effects. In general the amount of information provided by the phenotype about the genetic predisposition is determined by the narrow sense heritability. Note that heritability is population-specific and a heritability of 0 does not necessarily imply that there is no genetic determinism for the trait. The trait might be highly influenced by genetic factors. Yet, the observed variation for the trait might not be due to genetic factors, because all alleles contributing to the trait are fixed, and there are no segregating causal alleles for the trait, in the population. Therefore, observed variation may only be due to environmental factors, and the heritability in that population might be 0. 

__Genetic correlation__ is the proportion of variance that two traits share due to genetic causes. Genetic correlations are not the same as heritability, as it is about the overlap between the two sets of influences and not the absolute magnitude of their respective genetic effects; two traits could be both highly heritable but not be genetically correlated, or they could have small heritabilities and be completely correlated (as long as the heritabilities are non-zero).
Genetic correlation ($\rho_a$) is the genetic covariance between two traits divided by the product of genetic standard deviation for each of the traits:
\begin{align}
\rho_{g_{12}}=\frac{\sigma_{g_{12}}}{\sqrt{\sigma_{g_{1}}^2 \sigma_{g_{2}}^2}}
\end{align}
where $\sigma_{g_{12}}$ is the genetic covariance and  $\sigma_{g_{1}}^2$ and $\sigma_{g_{2}}^2$ are the genetic variances for the two traits in the population. 
A genetic correlation of 0 implies that the genetic effects on one trait are independent of the other, while a correlation of 1 implies that all of the genetic influences on the two traits are identical. 
Thus in order to estimate the heritability and genetic correlation we need to estimate the variance component defined above ($\sigma_g^2$ and $\sigma_e^2$), for each trait, in addition to the genetic covariance between traits. 


## Data required for estimating genetic parameters
Information on phenotypes and genetic relationships for individuals in a study population are, in combination with appropriate statistical models, used for accurate estimation of genetic parameters and genetic predisposition of individuals.

__Phenotypes__ for traits of importance for human health and disease need to be recorded accurately and completely.If individuals are selectively recorded (e.g. case-control study) the genetic parameters estimated should be adjusted. Data should include factors that could influence the trait phenotype. Observations should be objectively measured, if at all possible.

__Genetic relationships__ for the individuals in the study population are required. Genetic relationships can be inferred from a pedigree or, alternatively, computed from genetic markers. Individuals and their parents need to be uniquely identified in the data. 

Information about development (e.g., birth dates), ancestry, and genotypes for various markers could also be stored. If individuals are not uniquely identified, then genetic analysis of the population may not be possible at the individual level. 

Prior information about the traits is useful. Read the literature. Most likely other researchers have already made analyses of the same traits. Even though their study populations are not the same as yours, their models could be useful starting points for further analyses. Their parameter estimates could result in useful predictions. The idea is to avoid the pitfalls and problems that other researchers have already encountered. 


## Statistical models and variance components
For estimating genetic parameters we need to specify a statistical model that describes the genetic and non-genetic factors that may affect the trait phenotypes. Often the non-genetic factors are referred to as systematic effect such as age, sex, or year:
\begin{align}
			\text{phenotype}=\text{mean} + \text{systematic effect} + \text{genetic effect}  + residual	 \notag
\end{align}
Here we make a distinction between fixed effects, that determine the level (expected mean) of observations, and random effects that determine variance. A model consists of at least one fixed effect (i.e. mean) and one random effect (the residual error variance). If observations also are influenced by a genetic contribution of the individuals, then a genetic variance component exists as well. In that situation, we have two components contributing to the total variance of the observations: a genetic and a residual variance component. 

\begin{comment}
A statistical model is usually specified as a mathematical relationship between one or more random variables and other non-random variables. In quantitative genetics it is often based on the additive genetic model specified above with inclusion of additional factors that may affect the trait of interest: 

\begin{align}
			y=\mu + \text{...systematic effect....} + a + e	 \notag
\end{align}

where  $\mu$ is the population mean, $a$ is the additive effect, and $e$ is the environmental deviation (or residual) not explained by the systematic effects and the genetic effects in the model. 

We assume that the additive genetic effect, a, and the residual term, e, are normally distributed such that $a \sim N(0,\sigma^2_{a})$ and $e \sim N(0,\sigma^2_{e})$. The goal is to derive estimates for the variance components that is the additive genetic variance $\sigma^2_{a}$, and the residual variance $\sigma^2_{e}$ the residual variance. 
\end{comment}

The statistical model is a formal representation of our quantitative genetic theory, but it is important to realize that all models are simple approximations of how genetic and non-genetic factors influence a trait. The goal of the statistical analysis is to find the best practical model that explains the most variation in the data. Statistical knowledge is required. The methods used for estimating genetic parameters is based on statistical concepts such as random variables, matrix algebra, multivariate normal theory, and linear (mixed) models. These concepts and their use will be explained in the following sections. 

## When to estimate variance components
In general, the estimation of variance components has to be based on a sufficient 
amount of data. Depending on the data structure and measurements, estimations can be based on hundreds (in selection experiments) or more 
than 10,000 observations (in field recorded data). Importantly, in cases where the data set is small, the information from the literature may yield more accurate estimates of variance components. In general, we have 
to estimate variance without external information if we study a new trait, for which no prior parameter estimates are available, or a different sample: variances and covariances might have changed over time, or due to various evolutionary forces (genetic drift, selection, migration, or mutation).

Generally, it is assumed that variances and covariances, and especially their ratio (like heritability, correlation) do not rapidly change over time. However, it is well known that the genetic variance changes as a consequence of selection or genetic drift. Changes are expected, especially when generation intervals are short, selection intensity is high, or the trait under selection is determined by few causal genes with large effects. Moreover, the circumstances under which measurements are taken can change. If measurement conditions are better controlled, and getting more uniform over time, the environmental variance decreases, and consequently the heritability increases. Finally, the biological basis of a trait may change from one environment to another. In conclusion, there are 
sufficient reasons for regular estimation of (co-)variance components. 

## Methods for estimation of genetic parameters
In general, estimation of heritability and genetic correlation are based on methods that determine resemblance between genetically related individuals. Close (compared to distant) relatives share more alleles and, if the trait is under genetic influence, they will therefore share phenotypic similarities. Methods for estimating heritability include parent-offspring regression, analysis of variance (ANOVA) for family data (e.g., half-sib/full-sib families) and restricted maximum likelihood (REML) analysis for a general pedigree. These methods are increasingly more complex, but they are also increasingly more flexible. While REML can analyze any type of relationships and structures, ANOVA can only analyze groups of individuals with similar relationships (e.g., half-sib, or full-sib families), and regression analysis can only analyze pairs of individuals with similar relationships (e.g., pairs of parent and respective offspring, or pairs of monozygotic twins). 

# Estimating genetic parameters for a general pedigree using REML
Genetic parameters are nowadays estimated using restricted maximum likelihood (REML) or Bayesian methods. These methods allow for estimation of genetic parameters using phenotypic information for individuals from a general pedigree (with arbitrary relationships among them). REML is based on linear mixed model methodology and uses a likelihood approach to estimate genetic parameters.


## Linear mixed model:
The linear mixed model contains the observation vector for the trait(s) of interest ($y$), the 'fixed effects' that explain systematic differences in $y$, and the 'random effects' which capture unidentified factors affecting $y$, e.g., random genetic effects and random residual effects.

A matrix formulation of a general model equation is:
\begin{align}
y &= Xb + Za + e \notag
\end{align}

where
\begin{align}
y &: \text{is the vector of observed values of the trait(s),} \notag \\
b &: \text{is a vector of factors, collectively known as fixed effects,} \notag \\
a &: \text{is a vector of factors known as random additive genetic effects,} \notag \\
e &: \text{is a vector of residual terms, also random,} \notag \\
X &: \text{is a known design matrix that relates the elements of b to their corresponding element in y.} \notag \\
Z &: \text{is a known design matrix that relates the elements of a to their corresponding element in y.} \notag 
\end{align}

\begin{comment}
The __observation vector__ contains elements resulting from measurements, either subjective or objective, on the experimental units (usually individuals) under study. The elements in the observation vector are random variables that have a multivariate distribution, and if the form of the distribution is known, then advantage should be taken of that knowledge. Usually $y$ is assumed to have a multivariate normal distribution, but that is not always true. The elements of $y$ should represent random samples of observations from some defined population. If the elements are not randomly sampled, then bias in the estimates of $b$ and $a$ can occur, which would lead to errors in ranking individuals based on genetic predisposition.
\end{comment}

The factors (or 'variables') which describe fixed and random effects, may be either continuous or categorical. 

__Continuous variables__ have (theoretically) an infinite range of
possible values (e.g., body weight or height in humans).

__Categorical variables__ fall in distinct categories (e.g., different years). These variables do not describe a gradient of values along a single axis, like height of individuals (values between 0 and "infinity"). Instead, they have distinct classes (or 'levels'), each of which has its own estimated effect.

In addition to continuous or categorical (factor), it is necessary to distinguish between __fixed__ and __random__ effects in the linear mixed model. 

__Fixed effect:__ If the number of levels of a categorical variables is small or limited to a fixed number, and inferences about that factor are going to be limited to that set of levels, and to no others, then its effects is usually fixed. In other words, if a new sample of observations is made (from a new experiment), and the same levels of that factor are in both samples, then the factor is usually fixed. Continuous variables are usually fixed too (but not always).

__Random effect:__ If the number of levels of a categorical variable is large, then that factor may be random. If the inferences about that factor are going to be made for an entire population of levels, and if the levels of the factor are a sample from an infinitely large population, then that factor is usually random. In other words, if a new sample of observations are made (from a new experiment), and the levels are completely different between the two samples, then the factor is usually random.

## Expectation and variance of variables in the linear mixed model:
In the statistical model (specified above) the random effects ($a$ and $e$) and the phenotypes ($y$) are considered to be random variables which follow a multivariate normal distribution. In general terms the expectations of these random variables are:  
\begin{align}
E(y) &= E(Xb) + E(Za) + E(e) \notag \\
     &= Xb + 0 + 0 \notag \\
     &= Xb \notag
\end{align}
and the variance-covariance matrices are:
\begin{align}
 Var(a) &= G \notag \\
 Var(e) &= R \notag \\
 Var(y) &= G + R = V \notag
\end{align}
 
where $G$, $R$ and $V$ are square matrices of genetic, residual and phenotypic (co)variances among the individuals, respectively. 
For a single trait model these covariances can be written as:
\begin{align}
 Var(a) &= A\sigma_a^2 \notag \\
 Var(e) &= I\sigma_e^2 \notag
\end{align}
where $A$ is the additive genetic relationship matrix. 
For a multiple trait model:
\begin{align}
 Var(a) &= A\otimes V_a \notag \\
 Var(e) &= I\otimes V_e \notag
\end{align}
where $V_a$ and $V_e$ are square matrcies of genetic and residual (co)variances among traits, respectively. To simplify we have assumed one record per individual per trait and therefore the $Z$ matrix reduces to an identity matrix which can be left out of the equation system.


## Genetic relationships among individuals 
Estimating narrow sense heritability using REML (similar to the parent-offspring regression and ANOVA methods) requires that the phenotypic covariance between related individuals can be expressed by their additive genetic relationship and the additive genetic variance ($\sigma_a^2$). Related individuals share more alleles and thus resemble each other (have correlated phenotypes, to an extent that depends on additive genetic relationships). 

In general, the genetic covariance between individuals depends on the additive genetic relationship. Examples of different types of additive genetic relationships can be found in the table below. The additive genetic relationship ($A_{ij}$) between the various sources (j) and the individual itself (i) can be seen in the table below.

\begin{center} 
\begin{tabular}{|c|c|}
  \hline
  Relative  &  $A_{ij}$\\
  \hline
  Self  &  1.0    \\
  \hline
  Unrelated  &  0    \\
  \hline
  Mother  &  0.5 \\
  \hline
  Father  &  0.5 \\
  \hline
  Grandparent  &  0.25 \\
  \hline
  Half-sib  &  0.25 \\
  \hline
  Full-sib  &  0.5 \\
  \hline
  Cousin  &  0.0625 \\
  \hline
  Child  &  0.5 \\
  \hline
  Twin(MZ/DZ)  &  1/0.5 \\
  \hline
\end{tabular}
\end{center}

The $A$ matrix expresses the additive genetic relationship among individuals in a population, and is called the __numerator relationship matrix__ $A$. The matrix $A$ is symmetric and its diagonal elements $A_{ii}$ are equal to $1 + F_i$ where $F_i$ is the __coefficient of inbreeding __ of individual $i$. $F_i$ is defined as the probability that two alleles taken at random from individual $i$ are identical by descent. As such, $F_i$ is also the kinship coefficient of its parents (half their genetic relationship).

Each off-diagonal elements $(A_{ij})$ is the genetic relationship between individuals $i$ and $j$. Multiplying the matrix $A$ by the additive genetic variance $\sigma_a^2$ leads to the covariance among breeding values. Thus if $a_i$ is the breeding value of individual $i$ then 

\begin{equation}
var(a_i) = A_{ii} \sigma_a^2 = (1 + F_i) \sigma_a^2
\end{equation}


### Algorithm to compute the numerator relationship matrix $A$ {#algorithmtocomputea}
The matrix $A$ can be computed using a recursive method. In what follows the recursive method to compute the components of $A$ is described. Initially, individuals in a pedigree are numbered from $1$ to $n$ and ordered such that parents precede their children. The following rules are then  used to compute the components of $A$. 

* If both parents $s$ and $d$ of individual $i$ are known, then 
    + the diagonal element $A_{ii}$ corresponds to: $A_{ii} = 1 + F_i = 1 + \frac{1}{2} A_{sd}$ and
    + the off-diagonal element $A_{ji}$ is computed as:  $A_{ji} = \frac{1}{2} (A_{js} + A_{jd})$
    + because $A$ is symmetric $A_{ji} = A_{ij}$
    
* If only one parent $s$ of individual $i$ is known and assumed unrelated to the mate
    + $A_{ii} = 1$
    + $A_{ij} = A_{ji} = \frac{1}{2} (A_{js})$
    
* If both parents are unknown    
    + $A_{ii} = 1$
    + $A_{ij} = A_{ji} = 0$
    

#### Numeric Example
```{r pedexamplesetup, echo=FALSE, results='hide'}
suppressPackageStartupMessages( library(pedigreemm) )
n_nr_ani_ped <- 6
n_nr_parent <- 2
tbl_ped <- tibble::tibble(Child = c((n_nr_parent+1):n_nr_ani_ped),
                             Father = c(1, 1, 4, 5),
                             Mother  = c(2, NA, 3, 2))
ped <- pedigree(sire = c(rep(NA, n_nr_parent), tbl_ped$Father), dam = c(rep(NA, n_nr_parent), tbl_ped$Mother), label = as.character(1:n_nr_ani_ped))
matA <- as.matrix(getA(ped = ped))
matAinv <- as.matrix(getAInv(ped = ped))
```

We are given the following pedigree and we want to compute the matrix $A$ using the recursive method described in \@ref(algorithmtocomputea). 

```{r tabpedexample, echo=FALSE, results='asis'}
knitr::kable(tbl_ped,
             format = 'latex',
             booktabs = TRUE,
             longtable = TRUE,
             caption = "Example Pedigree To Compute Additive Genetic Relationship Matrix")
```

The first step of the computations of $A$ are the numbering and the ordering of all the individuals. This is already done in the pedigree shown in Table \@ref(tab:tabpedexample). The components of $A$ are computed row-by-row starting with $A_{11}$. 

\begin{align}
A_{11} &= 1 + F_1 = 1 + 0 = 1 \notag \\
A_{12} &= 0 = A_{21} \notag \\
A_{13} &= \frac{1}{2} (A_{11} + A_{12}) = 0.5 = A_{31} \notag \\
A_{14} &= \frac{1}{2} A_{11} = 0.5 = A_{41}  \notag \\
A_{15} &= \frac{1}{2} (A_{14} + A_{13}) = 0.5 = A_{51} \notag \\
A_{16} &= \frac{1}{2} (A_{15} + A_{12}) = 0.25 \notag
\end{align}

The same computations are also done for all the other components of the matrix $A$. The final result for the matrix looks as follows

```{r displaymatrixa, echo=FALSE, results='asis'}
cat("$$\n")
# cat("A = \\left[")
# cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = matA, pnDigits = 4), sep = "\n"), "\n")
# cat("\\right]\n")
cat(paste(rmdhelp::bmatrix(pmat = matA, ps_name = 'A'), sep = '\n'), '\n')
cat("$$\n")
```

As a result, we can see from the components of the above shown matrix $A$ that individuals $1$ and $2$ are not related to each other. Furthermore from the diagonal elements of $A$, it follows that individuals $5$ and $6$ are inbred while individuals $1$ to $4$ are not inbred. Finally, we can see that different types of relationships were included in this data. In comparison, only two types of relationships could exist in regression and ANOVA analyses: unrelated (e.g., $A_{ij}$=0 between individuals from different families) or not (e.g., $A_{ij}$=0.5 between individuals from the same full-sib family). 


## Restricted Maximum Likelihood approach for variance component estimation
Restricted Maximum Likelihood is a method that is used to estimate the parameters (i.e. variance components $\sigma_{a}^2$ and $\sigma_{e}^2$) in the linear mixed model specified above. The general principle used in maximum likelihood methods is to find the set of parameters which maximizes the likelihood of the data, i.e., the probability of observations given the model and its parameter estimates $p(y|\hat{\theta})$. For a single trait model including additive genetic effects the vector of parameters can be specified as $\hat{\theta}=\hat{b}, \hat{\sigma}^2_{a}, \hat{\sigma}^2_{e}$. 

It is useful to recall that the likelihood $L(\theta|{y})$ may be any function of the parameters ($\theta$) that is proportional to $p({y}|\theta)$. Maximizing $L(\theta|{y})$ leads to obtaining the most likely value of $\theta$ ($\hat{\theta}$) given the data ${y}$. The REML method was developed by \citet{Patterson1971} as an improvement of the standard Maximum Likelihood (ML). ML assumes that fixed effects are known without error which is in most cases false and, as consequence, it produces biased estimates of variance components (usually, the residual variance is biased downward). As a solution to this problem, REML estimators maximize only the part of the likelihood which does not depend on the fixed effects, and REML, by itself, does not estimate the fixed effects. 
There are no simple one-step solutions for estimating the variance components based on REML \citep{LynchWalsh1998}. Instead, the partial derivatives of the likelihoods are inferred with respect to the variance components. The solutions to these involve the inverse of the variance-covariance matrix, which themselves includes the variance components, so the variance components estimates are non-linear functions of the variance components. It is therefore necessary to apply iterative methods to obtain the estimates.

### Estimates of genetic parameters
From the REML estimate of the variance components estimates of the narrow sense heritability can easily be computed by 
\begin{align}
\hat{h}^2 &= \hat{\sigma}^2_{a}/(\hat{\sigma}^2_a+\hat{\sigma}^2_e)
\end{align}
where the the hat ($\hat{~}$) refers to estimators. Similar estimates for the genetic correlation ($\rho_a$) is the genetic covariance between two traits divided by the product of genetic standard deviation for each of the traits:
\begin{align}
\hat{\rho}_{a_{12}}=\frac{\hat{\sigma}_{a_{12}}}{\sqrt{\hat{\sigma}_{a_{1}}^2 \hat{\sigma}_{a_{2}}^2}}
\end{align}

### Statistical test of variance components and genetic parameters
The concept of likelihood also provides a framework for testing hypotheses regarding, for example, the variance components in the models. In particular, the so-called likelihood ratio tests are used to assess whether a reduced model fits the data better than a full model by comparing the likelihoods of the two models. 

The LR test statistic can be derived by using the following formula:

\begin{equation}
T_\textrm{LRT} = 2 \ln\left[\frac{L(\hat{\theta}|\vect{y})}{L(\hat{\theta}_r|\vect{y})}\right] 
		= -2\left[l(\hat{\theta}_r|\vect{y})-l(\hat{\theta}|\vect{y})\right]
\end{equation}

where $l(\hat{\theta}|{y})$ is the log-likelihood for the full model, and $l(\hat{\theta}_r|{y})$ is the log-likelihood for the reduced model.

When the sample size is sufficiently large, the LR statistic is $\chi^2$ distributed with $\kappa$ degrees of freedom, where $\kappa$ parameters that were free in the full model, have been assigned fixed values in the reduced.

A high likelihood ratio shows that the full model with two (different) variance components is better at explaining the observed phenotypic variance
than the reduced model with only one variance component.

It is fundamental for the reduced model to be nested in the full model, otherwise this approach does not make any sense. When the REML procedure is used, it is also important for the two models being compared to have the same fixed effects, otherwise the two likelihoods are not comparable, as can be easily understood by looking at the concept of restricted likelihood.  


## Advantages of using REML for estimating genetic parameters
Although REML does not produce unbiased estimates, it is still the method of choice due to the fact that this source of bias is also present but higher in ML estimates \citep{LynchWalsh1998}.

REML requires that $y$ have a multivariate normal distribution 
although various authors have indicated that ML or REML estimators may be an 
appropriate choice even if normality does not hold (Meyer, 1990). 

REML can account for selection when the complete mixed model is used with 
all genetic relationships and all data used for selection is included (Sorensen and Kennedy, 1984; Van der Werf and De Boer, 1990). 

There is obviously an advantage in using (RE)ML methods that are more flexible in 
handling several (overlapping) generations (and possibly several random effects). However, the use of such methods are "dangerous" in the sense we no longer need to think explicitly about the data structure. For example, to estimate additive genetic variance, we need to have a data set that contains a certain family structure which allows us to separate differences between families from differences within families. Or in other words, we need to differentiate genetic and residual effects, so the structure due to genetic relationships must be different from the structure due to residual effects (i.e., the G and R matrices must be different enough). 

Today, heritability can be estimated based on genetic relationships, inferred from general pedigrees or estimated from genetic markers. Linear mixed models are also used in genetic evaluation, allowing information on all known relationships between individuals to be incorporated simultaneously in the analysis. Linear mixed models can include additional effects to describe the data more accurately: maternal, permanent environmental, cytoplasmic or dominance effects and QTL effects. These effects may be fitted as additional random effects. 





\begin{comment}

# Variance Components {#variance-components}
The prediction of breeding values using a BLUP animal model required the __variance components__ $\sigma_e^2$ for the residual variance and $\sigma_u^2$ for the genetic additive variance to be known. For the sire model, $\sigma_u^2$ is replaced by the sire variance component $\sigma_s^2$. In real world livestock breeding evaluations, these variance components are not known and hence must be estimated from the data. The data analysis procedure that estimates the variance components from data is called __variance components estimation__. 


## Sire Model
The sire model is used to motivate the introduction of the topic of variance components estimation. The sire model is given by

\begin{equation}
y = X\beta + Z_ss + e
(\#eq:varcompsiremodel)
\end{equation}

with $var(e) = R$, $var(s) = A_s \sigma_s^2$ and $var(y) = Z_sA_sZ_s^T \sigma_s^2 + R$. The matrix $A_s$ is the numerator relationship for sires, the sire variance component $\sigma_s^2$ corresponds to $0.25 * \sigma_u^2$ and $R$ can often be simplified to $R = I * \sigma_e^2$. The interest in this chapter is how to estimate $\sigma_s^2$ and $\sigma_e^2$. 

In the simple case the vector $\beta$ is reduced to just one scalar fixed effects parameter. This reduced $X$ to a matrix with one column with all elements equal to $1$. Assuming that we have $q$ unrelated sires the relationship matrix $A_s$ for the sires corresponds to the identity matrix $I$. 


## Analysis Of Variance (Anova)
As a first approach we can use an analysis of variance by fitting 

1. a model with an overall effect $\beta = \mu$ and 
2. a model with sire effects. 

These two models give an analysis of variance of the following structure

\begin{tabular}{lll}
\hline \\
Source           &  Degrees of Freedom ($df$)          &  Sums of Squares ($SSQ$) \\
\hline \\
Overall ($\mu$)  &  $Rank(X)=1$                        &  $y^TX(X^TX)^{-1}X^Ty = F$  \\
Sires ($s$)      &  $Rank(Z_s) - Rank(X) = q - 1$      &  $y^TZ_s(Z_s^TZ_s)^{-1}Z_s^Ty - y^TX(X^TX)^{-1}X^Ty = S$  \\
Residual ($e$)   &  $n - Rank(Z_s) = n - q$            &  $y^Ty - y^TZ_s(Z_s^TZ_s)^{-1}Z_s^Ty = T$ \\
\hline \\
Total            &  $n$                                &  $y^Ty$ \\
\hline
\end{tabular}

The sums of squares ($SSQ$) can also be expanded into sums of scalar quantities which might be easier to understand. For our sire model we get

$$F = y^TX(X^TX)^{-1}X^Ty = \frac{1}{n} \left[\sum_{i=1}^n y_i \right]^2$$
where $n$ corresponds to the number of observations in the dataset.

$$S= y^TZ_s(Z_s^TZ_s)^{-1}Z_s^Ty - y^TX(X^TX)^{-1}X^Ty = \sum_{i=1}^{q} \frac{1}{n_i} \left[\sum_{j=1}^{n_i} y_{ij}\right]^2 - F $$
where $n_i$ corresponds to the number of observations for sire $i$. 

$$T = y^Ty - y^TZ_s(Z_s^TZ_s)^{-1}Z_s^Ty = \sum_{i=1}^n y_i^2 - S - F$$

In principle effects $\beta$ and $s$ are treated as fixed effects in the above anova. If estimates of $\sigma_e^2$ and $\sigma_s^2$ are required the observed sums of squares $S$ and $T$ can be equated to their expected values $E(T) = (n-q) \sigma_e^2$ and $E(S) = (q-1) \sigma_e^2 + tr(Z_sMZ_s)\sigma_s^2$ where $M = I - X(X^TX)^{-1}X^T$ and $tr(M)$ stands for the trace of matrix M which corresponds to the sum of the diagonal elements of matrix $M$.



## Numerical Example
We want to show the estimation of variance components with a very small data set. The data that will be used is shown in the table below. The observations consist of pre-weaning weight gains of beef cattle. 


```{r datavcesm, echo=FALSE, results='asis'}
tbl_num_ex_chp12 <- tibble::tibble( Animal = c(4, 5, 6, 7),
                                        Sire   = c(2, 1, 3, 2),
                                        WWG    =  c(2.9, 4.0, 3.5, 3.5) )

knitr::kable(tbl_num_ex_chp12,
             booktabs  = TRUE,
             longtable = TRUE,
             caption   = "Small Example Dataset for Variance Components Estimation Using a Sire Model")
```


The model used is a simplified sire model where all the fixed effect are captured by a common mean $\mu$. Then there is the sire effect $s$ as a random effect and the random residual effect. Hence for any given observation $y_{ij}$ for animal $i$ of sire $j$, we can write

$$y_{ij} = \mu + s_j + e_i$$

with $\mu$ the common mean, $s_j$ the random effect of sire $j$ ($j = 1, 2, 3$) and $e_i$ corresponds to the random residual of observation $i$ ($i = 1, \ldots, 4$). In matrix notation thi s model was already given in \@ref(eq:varcompsiremodel). The design matrix $X$ is a matrix with one column and with elements all equal to $1$. The design matrix $Z_s$ links observations to sire effects. 


```{r,echo=FALSE, results='asis'}
n_nr_obs_p02 <- nrow(tbl_num_ex_chp12)
### # design matrix X
mat_x_p02 <- matrix(1, nrow = n_nr_obs_p02, ncol = 1)
### # design matrix Z
mat_z_p02 <- matrix(c(0, 1, 0,
                      1, 0, 0,
                      0, 0, 1,
                      0, 1, 0), nrow = n_nr_obs_p02, byrow = TRUE)
n_nr_sire <- ncol(mat_z_p02)
### # Observations
mat_obs <- matrix(tbl_num_ex_chp12$WWG, ncol = 1)

cat("$$\n")
cat("X = \\left[\n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = mat_x_p02, pnDigits = 0), collapse = "\n"), "\n")
cat("\\right] \\text{, }")
cat("Z_s = \\left[\n")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = mat_z_p02, pnDigits = 0), collapse = "\n"), "\n")
cat("\\right]")
cat("$$\n")
```

```{r anovacomp, echo=FALSE, results='hide'}
### # compute F
ytx <- crossprod(mat_obs,mat_x_p02)
xtx <- crossprod(mat_x_p02)
ssqf <- ytx %*% solve(xtx) %*% t(ytx)
### # compute S
ytz <- crossprod(mat_obs, mat_z_p02)
ztz <- crossprod(mat_z_p02)
ssqs <- ytz %*% solve(ztz) %*% t(ytz) - ssqf
### # compute R
yty <- crossprod(mat_obs)
ssqr <- yty - ssqs - ssqf

```

An analysis of variance can be constructed as

\begin{center}
\begin{tabular}{lll}
\hline \\
Source           &  Degrees of Freedom ($df$)          &  Sums of Squares ($SSQ$) \\
\hline \\
Overall ($\mu$)  &  $Rank(X)=1$                  &  $F = `r ssqf`$  \\
Sires ($s$)      &  $Rank(Z_s) - Rank(X) = q - 1$  &  $S = `r ssqs`$  \\
Residual ($e$)   &  $n - Rank(Z_s) = n - q$        &  $T = `r ssqr`$ \\
\hline \\
\end{tabular}
\end{center}


With 

```{r varcompest, echo=FALSE, results='asis'}
mat_m <- diag(n_nr_obs_p02) - mat_x_p02 %*% solve(xtx) %*% t(mat_x_p02)
ztmz <- t(mat_z_p02) %*% mat_m %*% mat_z_p02
tr_ztmz <- sum(diag(ztmz))
cat("$$\n")
cat("M = \\left[")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = mat_m)))
cat("\\right] \\text{ and } ")
cat("Z_s^TMZ = \\left[")
cat(paste(rmddochelper::sConvertMatrixToLaTexArray(pmatAMatrix = ztmz)))
cat("\\right] \n")
cat("$$\n")
```

we get the following estimates

```{r, echo=FALSE}
hat_sigmae2 <- ssqr
hat_sigmas2 <- (ssqs - (n_nr_sire-1) * hat_sigmae2) / tr_ztmz
```

$$\hat{\sigma_e^2} = T = `r hat_sigmae2`$$
$$\hat{\sigma_s^2} = \frac{S - (q-1)\hat{\sigma_e^2}}{tr(Z_s^TMZ_s)} = \frac{`r ssqs` - `r n_nr_sire-1` * `r hat_sigmae2`}{`r tr_ztmz`} = `r hat_sigmas2`$$

The same computations based on an anova can be done in `R` very easily. Assume that our dataset is in a dataframe which is called `tbl_num_ex_chp12_aov`. We are doing the anova using the function `aov()` to get the sums of squares.

```{r, echo=FALSE, results='hide'}
tbl_num_ex_chp12_aov <- tbl_num_ex_chp12
tbl_num_ex_chp12_aov$Sire <- as.factor(tbl_num_ex_chp12_aov$Sire)
```

```{r, echo=TRUE, results='markup'}
aov_num_ex_chp12 <- aov(formula = WWG ~ Sire, data = tbl_num_ex_chp12_aov)
summary(aov_num_ex_chp12)
```


The results from above are obtained for $\hat{\sigma_e^2} = 0.18$ as the value under the column `Mean Sq` in the row `Residuals`. Because in our computations above, we have considered the estimation of the overall effect which is not done in the function `aov()` in R.


## Negative Estimates with Anova
One of the problems that frequently occurs when using anova to estimate variance components is that some estimates might be negative. Negative estimates are outside of the permissible range for the parameter and hence are not valid estimates. As a consequence of that alternative methods have been proposed to estimate variance components. 


## Likelihood-Based Approaches
The maximum likelihood (ML) approach was developed and popularized by R. A. Fisher. ML is a general approach for parameter estimation and is not only used for estimating variance components. Let us assume that our observed traits are continuous and real-valued quantities. In ML we assume that these quantities follow a certain density. This density is a function of the observed values and of unknown parameters that we want to estimate. 


### Density of Observations
Given a vector $y$ of observations. As already mentioned, the vector $y$ follows a certain density. As an example such a density might be a multivariate normal distribution. For a given vector $y$ of length $n$, the underlying $n$-dimensional multivariate normal distribution has the following form

$$
f_Y(y) = \frac{1}{\sqrt{(2\pi)^n det(\Sigma)}} exp \left\{-\frac{1}{2}(y - \mu)^T \Sigma^{-1}(y - \mu) \right\}
$$

\begin{tabular}{lll}
with  &  $\mu$  &  expected value of $y$ \\
      &  $\Sigma$  &  variance-covariance matrix of $y$ \\
      &  $det()$   &  determinant
\end{tabular}


### Likelihood Function
As already mentioned the density is a function of the observed data $y$ and of some unknown parameters. For the multivariate normal distribution these parameters are $\mu$ and $\Sigma$. Before observing any data, we can interpret the density $f(y | \mu, \Sigma)$ as a function of $y$ for some fixed values of $\mu$ and $\Sigma$. But once the data has been observed, $y$ is fixed and the parameters $\mu$ and $\Sigma$ are unknown and must be estimated from the data. For the task of parameter estimation, it makes more sense to view $f(y | \mu, \Sigma)$ as a function of $\mu$ and $\Sigma$. We can write this function a little different

$$L(\mu, \Sigma) = f(y | \mu, \Sigma)$$

The function $L(\mu, \Sigma)$ is called the __Likelihood__ function. 


### Maximum Likelihood
For a given dataset we choose an appropriate density which is suitable for our observations. As already mentioned, due to the Central Limit Theorem, the normal distribution is often used as a density for observations. Once, we have chosen the density, it contains unknown parameters which we have to estimate from the data. Loosely speaking, our goal is to determine the parameters such that the observed data is modeled as good as possible. This requirement is translated into a mathematical framework by the maximization of the likelihood. Hence for a given dataset our parameter estimates are determined such that the likelihood is maximized. For our multi-variate normal distribution, this can be transformed into the following equations

$$\hat{\mu} = argmax_{\mu} L(\mu, \Sigma)$$

and

$$\hat{\Sigma} = argmax_{\Sigma} L(\mu, \Sigma)$$


## Summary
The topic of variance component estimation is a huge area. We have just covered two possible approaches to get estimates of variance components. There are many more of them. The coverage of these methods is outside of the scope of this course.

\end{comment}
